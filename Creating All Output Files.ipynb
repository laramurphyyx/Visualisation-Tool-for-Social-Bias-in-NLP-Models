{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b49e696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from transformers import AlbertTokenizer, AlbertForMaskedLM\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForMaskedLM\n",
    "from transformers import DistilBertTokenizer, DistilBertForMaskedLM\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "from crows_pairs_methods import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f79b4d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_types = [\n",
    "    'race-color',\n",
    "    'gender',\n",
    "    'socioeconomic',\n",
    "    'nationality',\n",
    "    'religion', \n",
    "    'age',\n",
    "    'sexual-orientation',\n",
    "    'physical-appearance',\n",
    "    'disability'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73cd17f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_models = [\n",
    "    'bert-base-cased',\n",
    "    'bert-base-uncased',\n",
    "    'bert-large-uncased',\n",
    "    'bert-large-cased',\n",
    "    'bert-base-multilingual-uncased',\n",
    "    'bert-base-multilingual-cased',\n",
    "    'allenai/scibert_scivocab_uncased',\n",
    "    'emilyalsentzer/Bio_ClinicalBERT',\n",
    "    'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract',\n",
    "    'nlpaueb/legal-bert-base-uncased',\n",
    "    'GroNLP/hateBERT',\n",
    "    'anferico/bert-for-patents',\n",
    "    'jackaduma/SecBERT'\n",
    "]\n",
    "\n",
    "ALBERT_models = [\n",
    "    'albert-base-v1',\n",
    "    'albert-base-v2'\n",
    "]\n",
    "\n",
    "ROBERTA_models = [\n",
    "    'roberta-base',\n",
    "    'distilroberta-base',\n",
    "    'roberta-large',\n",
    "    'huggingface/CodeBERTa-small-v1',\n",
    "    'climatebert/distilroberta-base-climate-f'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddd7b804",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_1 = BERT_models[:6]\n",
    "batch_2 = BERT_models[6:]\n",
    "batch_3 = ALBERT_models + ROBERTA_models + ['xlm-roberta-base', 'distilbert-base-multilingual-cased']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf10150",
   "metadata": {},
   "source": [
    "# Downloading Output Files for All Models Using Updated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1df04c55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1508/1508 [50:19<00:00,  2.00s/it]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1508/1508 [48:02<00:00,  1.91s/it]\n",
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1508/1508 [2:31:39<00:00,  6.03s/it]\n",
      "Some weights of the model checkpoint at bert-large-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1508/1508 [2:40:48<00:00,  6.40s/it]\n",
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1508/1508 [1:14:49<00:00,  2.98s/it]\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1508/1508 [1:24:06<00:00,  3.35s/it]\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "for model_name in batch_1:\n",
    "\n",
    "    # supported masked language models (using bert)\n",
    "    if model_name in BERT_models:\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        model = BertForMaskedLM.from_pretrained(model_name)\n",
    "    elif model_name in ALBERT_models:\n",
    "        tokenizer = AlbertTokenizer.from_pretrained(model_name)\n",
    "        model = AlbertForMaskedLM.from_pretrained(model_name)\n",
    "    elif model_name in ROBERTA_models:\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "        model = RobertaForMaskedLM.from_pretrained(model_name)\n",
    "    elif model_name == 'xlm-roberta-base':\n",
    "        tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n",
    "        model = XLMRobertaForMaskedLM.from_pretrained(model_name)\n",
    "    elif model_name == 'distilbert-base-multilingual-cased':\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "        model = DistilBertForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # load data into panda DataFrame\n",
    "    df_data = read_data(\"fixed_data.csv\")\n",
    "\n",
    "    mask_token = tokenizer.mask_token\n",
    "    log_softmax = torch.nn.LogSoftmax(dim=0)\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    with open(\"bert\" + \".vocab\", \"w\") as f:\n",
    "        f.write(json.dumps(vocab))\n",
    "\n",
    "    lm = {\"model\": model,\n",
    "          \"tokenizer\": tokenizer,\n",
    "          \"mask_token\": mask_token,\n",
    "          \"log_softmax\": log_softmax,\n",
    "          \"uncased\": True\n",
    "    }\n",
    "\n",
    "    # score each sentence. \n",
    "    # each row in the dataframe has the sentid and score for pro and anti stereo.\n",
    "    df_score = pd.DataFrame(columns=['sent_more', 'sent_less', \n",
    "                                     'sent_more_score', 'sent_less_score',\n",
    "                                     'score', 'stereo_antistereo', 'bias_type'], dtype=object)\n",
    "\n",
    "    total = len(df_data.index)\n",
    "    with tqdm(total=total) as pbar:\n",
    "        for index, data in df_data.iterrows():\n",
    "            direction = data['direction']\n",
    "            bias = data['bias_type']\n",
    "            score = mask_unigram(data, lm)\n",
    "\n",
    "            for stype in score.keys():\n",
    "                score[stype] = round(score[stype], 3)\n",
    "\n",
    "            pair_score = 0\n",
    "            pbar.update(1)\n",
    "            if direction == 'stereo':\n",
    "                if score['sent1_score'] > score['sent2_score']:\n",
    "                    pair_score = 1\n",
    "            elif direction == 'antistereo':\n",
    "                if score['sent2_score'] > score['sent1_score']:\n",
    "                    pair_score = 1\n",
    "\n",
    "            sent_more, sent_less = '', ''\n",
    "            if direction == 'stereo':\n",
    "                sent_more = data['sent1']\n",
    "                sent_less = data['sent2']\n",
    "                sent_more_score = score['sent1_score']\n",
    "                sent_less_score = score['sent2_score']\n",
    "            else:\n",
    "                sent_more = data['sent2']\n",
    "                sent_less = data['sent1']\n",
    "                sent_more_score = score['sent2_score']\n",
    "                sent_less_score = score['sent1_score']\n",
    "\n",
    "            df_score = df_score.append({'sent_more': sent_more,\n",
    "                                        'sent_less': sent_less,\n",
    "                                        'sent_more_score': sent_more_score,\n",
    "                                        'sent_less_score': sent_less_score,\n",
    "                                        'score': pair_score,\n",
    "                                        'stereo_antistereo': direction,\n",
    "                                        'bias_type': bias\n",
    "                                      }, ignore_index=True)\n",
    "\n",
    "    file_path = 'All Output Files/Updated CrowS-Pairs Dataset/' + model_name.split('/')[-1].lower() + '.csv'\n",
    "    df_score.to_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "767594a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1508/1508 [56:07<00:00,  2.23s/it]\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1508/1508 [52:34<00:00,  2.09s/it]\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1508/1508 [59:24<00:00,  2.36s/it]\n",
      "Some weights of the model checkpoint at nlpaueb/legal-bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1508/1508 [57:00<00:00,  2.27s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1508/1508 [49:33<00:00,  1.97s/it]\n",
      "Some weights of the model checkpoint at anferico/bert-for-patents were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1508/1508 [2:38:49<00:00,  6.32s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1508/1508 [37:26<00:00,  1.49s/it]\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "for model_name in batch_2:\n",
    "\n",
    "    # supported masked language models (using bert)\n",
    "    if model_name in BERT_models:\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        model = BertForMaskedLM.from_pretrained(model_name)\n",
    "    elif model_name in ALBERT_models:\n",
    "        tokenizer = AlbertTokenizer.from_pretrained(model_name)\n",
    "        model = AlbertForMaskedLM.from_pretrained(model_name)\n",
    "    elif model_name in ROBERTA_models:\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "        model = RobertaForMaskedLM.from_pretrained(model_name)\n",
    "    elif model_name == 'xlm-roberta-base':\n",
    "        tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n",
    "        model = XLMRobertaForMaskedLM.from_pretrained(model_name)\n",
    "    elif model_name == 'distilbert-base-multilingual-cased':\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "        model = DistilBertForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # load data into panda DataFrame\n",
    "    df_data = read_data(\"fixed_data.csv\")\n",
    "\n",
    "    mask_token = tokenizer.mask_token\n",
    "    log_softmax = torch.nn.LogSoftmax(dim=0)\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    with open(\"bert\" + \".vocab\", \"w\") as f:\n",
    "        f.write(json.dumps(vocab))\n",
    "\n",
    "    lm = {\"model\": model,\n",
    "          \"tokenizer\": tokenizer,\n",
    "          \"mask_token\": mask_token,\n",
    "          \"log_softmax\": log_softmax,\n",
    "          \"uncased\": True\n",
    "    }\n",
    "\n",
    "    # score each sentence. \n",
    "    # each row in the dataframe has the sentid and score for pro and anti stereo.\n",
    "    df_score = pd.DataFrame(columns=['sent_more', 'sent_less', \n",
    "                                     'sent_more_score', 'sent_less_score',\n",
    "                                     'score', 'stereo_antistereo', 'bias_type'], dtype=object)\n",
    "\n",
    "    total = len(df_data.index)\n",
    "    with tqdm(total=total) as pbar:\n",
    "        for index, data in df_data.iterrows():\n",
    "            direction = data['direction']\n",
    "            bias = data['bias_type']\n",
    "            score = mask_unigram(data, lm)\n",
    "\n",
    "            for stype in score.keys():\n",
    "                score[stype] = round(score[stype], 3)\n",
    "\n",
    "            pair_score = 0\n",
    "            pbar.update(1)\n",
    "            if direction == 'stereo':\n",
    "                if score['sent1_score'] > score['sent2_score']:\n",
    "                    pair_score = 1\n",
    "            elif direction == 'antistereo':\n",
    "                if score['sent2_score'] > score['sent1_score']:\n",
    "                    pair_score = 1\n",
    "\n",
    "            sent_more, sent_less = '', ''\n",
    "            if direction == 'stereo':\n",
    "                sent_more = data['sent1']\n",
    "                sent_less = data['sent2']\n",
    "                sent_more_score = score['sent1_score']\n",
    "                sent_less_score = score['sent2_score']\n",
    "            else:\n",
    "                sent_more = data['sent2']\n",
    "                sent_less = data['sent1']\n",
    "                sent_more_score = score['sent2_score']\n",
    "                sent_less_score = score['sent1_score']\n",
    "\n",
    "            df_score = df_score.append({'sent_more': sent_more,\n",
    "                                        'sent_less': sent_less,\n",
    "                                        'sent_more_score': sent_more_score,\n",
    "                                        'sent_less_score': sent_less_score,\n",
    "                                        'score': pair_score,\n",
    "                                        'stereo_antistereo': direction,\n",
    "                                        'bias_type': bias\n",
    "                                      }, ignore_index=True)\n",
    "\n",
    "    file_path = 'All Output Files/Updated CrowS-Pairs Dataset/' + model_name.split('/')[-1].lower() + '.csv'\n",
    "    df_score.to_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45a73707",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1508/1508 [42:27<00:00,  1.69s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1508/1508 [45:47<00:00,  1.82s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1508/1508 [53:31<00:00,  2.13s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1508/1508 [33:39<00:00,  1.34s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1508/1508 [2:34:22<00:00,  6.14s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1508/1508 [44:28<00:00,  1.77s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1508/1508 [33:29<00:00,  1.33s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1508/1508 [2:01:24<00:00,  4.83s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1508/1508 [59:33<00:00,  2.37s/it]\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "for model_name in batch_3:\n",
    "\n",
    "    # supported masked language models (using bert)\n",
    "    if model_name in BERT_models:\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        model = BertForMaskedLM.from_pretrained(model_name)\n",
    "    elif model_name in ALBERT_models:\n",
    "        tokenizer = AlbertTokenizer.from_pretrained(model_name)\n",
    "        model = AlbertForMaskedLM.from_pretrained(model_name)\n",
    "    elif model_name in ROBERTA_models:\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "        model = RobertaForMaskedLM.from_pretrained(model_name)\n",
    "    elif model_name == 'xlm-roberta-base':\n",
    "        tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n",
    "        model = XLMRobertaForMaskedLM.from_pretrained(model_name)\n",
    "    elif model_name == 'distilbert-base-multilingual-cased':\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "        model = DistilBertForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # load data into panda DataFrame\n",
    "    df_data = read_data(\"fixed_data.csv\")\n",
    "\n",
    "    mask_token = tokenizer.mask_token\n",
    "    log_softmax = torch.nn.LogSoftmax(dim=0)\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    with open(\"bert\" + \".vocab\", \"w\") as f:\n",
    "        f.write(json.dumps(vocab))\n",
    "\n",
    "    lm = {\"model\": model,\n",
    "          \"tokenizer\": tokenizer,\n",
    "          \"mask_token\": mask_token,\n",
    "          \"log_softmax\": log_softmax,\n",
    "          \"uncased\": True\n",
    "    }\n",
    "\n",
    "    # score each sentence. \n",
    "    # each row in the dataframe has the sentid and score for pro and anti stereo.\n",
    "    df_score = pd.DataFrame(columns=['sent_more', 'sent_less', \n",
    "                                     'sent_more_score', 'sent_less_score',\n",
    "                                     'score', 'stereo_antistereo', 'bias_type'], dtype=object)\n",
    "\n",
    "    total = len(df_data.index)\n",
    "    with tqdm(total=total) as pbar:\n",
    "        for index, data in df_data.iterrows():\n",
    "            direction = data['direction']\n",
    "            bias = data['bias_type']\n",
    "            score = mask_unigram(data, lm)\n",
    "\n",
    "            for stype in score.keys():\n",
    "                score[stype] = round(score[stype], 3)\n",
    "\n",
    "            pair_score = 0\n",
    "            pbar.update(1)\n",
    "            if direction == 'stereo':\n",
    "                if score['sent1_score'] > score['sent2_score']:\n",
    "                    pair_score = 1\n",
    "            elif direction == 'antistereo':\n",
    "                if score['sent2_score'] > score['sent1_score']:\n",
    "                    pair_score = 1\n",
    "\n",
    "            sent_more, sent_less = '', ''\n",
    "            if direction == 'stereo':\n",
    "                sent_more = data['sent1']\n",
    "                sent_less = data['sent2']\n",
    "                sent_more_score = score['sent1_score']\n",
    "                sent_less_score = score['sent2_score']\n",
    "            else:\n",
    "                sent_more = data['sent2']\n",
    "                sent_less = data['sent1']\n",
    "                sent_more_score = score['sent2_score']\n",
    "                sent_less_score = score['sent1_score']\n",
    "\n",
    "            df_score = df_score.append({'sent_more': sent_more,\n",
    "                                        'sent_less': sent_less,\n",
    "                                        'sent_more_score': sent_more_score,\n",
    "                                        'sent_less_score': sent_less_score,\n",
    "                                        'score': pair_score,\n",
    "                                        'stereo_antistereo': direction,\n",
    "                                        'bias_type': bias\n",
    "                                      }, ignore_index=True)\n",
    "\n",
    "    file_path = 'All Output Files/Updated CrowS-Pairs Dataset/' + model_name.split('/')[-1].lower() + '.csv'\n",
    "    df_score.to_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf1bbd2",
   "metadata": {},
   "source": [
    "# Downloading Output Files for All Models Using Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42192833",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_1 = [\n",
    "    'bert-base-multilingual-uncased',\n",
    "    'bert-base-multilingual-cased'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffa9adaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1508/1508 [1:17:54<00:00,  3.10s/it]\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1508/1508 [1:22:03<00:00,  3.26s/it]\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "for model_name in batch_1:\n",
    "\n",
    "    # supported masked language models (using bert)\n",
    "    if model_name in BERT_models:\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        model = BertForMaskedLM.from_pretrained(model_name)\n",
    "    elif model_name in ALBERT_models:\n",
    "        tokenizer = AlbertTokenizer.from_pretrained(model_name)\n",
    "        model = AlbertForMaskedLM.from_pretrained(model_name)\n",
    "    elif model_name in ROBERTA_models:\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "        model = RobertaForMaskedLM.from_pretrained(model_name)\n",
    "    elif model_name == 'xlm-roberta-base':\n",
    "        tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n",
    "        model = XLMRobertaForMaskedLM.from_pretrained(model_name)\n",
    "    elif model_name == 'distilbert-base-multilingual-cased':\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "        model = DistilBertForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # load data into panda DataFrame\n",
    "    df_data = read_data(\"crows_pairs_anonymized.csv\")\n",
    "\n",
    "    mask_token = tokenizer.mask_token\n",
    "    log_softmax = torch.nn.LogSoftmax(dim=0)\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    with open(\"bert\" + \".vocab\", \"w\") as f:\n",
    "        f.write(json.dumps(vocab))\n",
    "\n",
    "    lm = {\"model\": model,\n",
    "          \"tokenizer\": tokenizer,\n",
    "          \"mask_token\": mask_token,\n",
    "          \"log_softmax\": log_softmax,\n",
    "          \"uncased\": True\n",
    "    }\n",
    "\n",
    "    # score each sentence. \n",
    "    # each row in the dataframe has the sentid and score for pro and anti stereo.\n",
    "    df_score = pd.DataFrame(columns=['sent_more', 'sent_less', \n",
    "                                     'sent_more_score', 'sent_less_score',\n",
    "                                     'score', 'stereo_antistereo', 'bias_type'], dtype=object)\n",
    "\n",
    "    total = len(df_data.index)\n",
    "    with tqdm(total=total) as pbar:\n",
    "        for index, data in df_data.iterrows():\n",
    "            direction = data['direction']\n",
    "            bias = data['bias_type']\n",
    "            score = mask_unigram(data, lm)\n",
    "\n",
    "            for stype in score.keys():\n",
    "                score[stype] = round(score[stype], 3)\n",
    "\n",
    "            pair_score = 0\n",
    "            pbar.update(1)\n",
    "            if direction == 'stereo':\n",
    "                if score['sent1_score'] > score['sent2_score']:\n",
    "                    pair_score = 1\n",
    "            elif direction == 'antistereo':\n",
    "                if score['sent2_score'] > score['sent1_score']:\n",
    "                    pair_score = 1\n",
    "\n",
    "            sent_more, sent_less = '', ''\n",
    "            if direction == 'stereo':\n",
    "                sent_more = data['sent1']\n",
    "                sent_less = data['sent2']\n",
    "                sent_more_score = score['sent1_score']\n",
    "                sent_less_score = score['sent2_score']\n",
    "            else:\n",
    "                sent_more = data['sent2']\n",
    "                sent_less = data['sent1']\n",
    "                sent_more_score = score['sent2_score']\n",
    "                sent_less_score = score['sent1_score']\n",
    "\n",
    "            df_score = df_score.append({'sent_more': sent_more,\n",
    "                                        'sent_less': sent_less,\n",
    "                                        'sent_more_score': sent_more_score,\n",
    "                                        'sent_less_score': sent_less_score,\n",
    "                                        'score': pair_score,\n",
    "                                        'stereo_antistereo': direction,\n",
    "                                        'bias_type': bias\n",
    "                                      }, ignore_index=True)\n",
    "\n",
    "    file_path = 'All Output Files/Original CrowS-Pairs Dataset/' + model_name.split('/')[-1].lower() + '.csv'\n",
    "    df_score.to_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12be4a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1508/1508 [53:38<00:00,  2.13s/it]\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1508/1508 [50:33<00:00,  2.01s/it]\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1508/1508 [57:02<00:00,  2.27s/it]\n",
      "Some weights of the model checkpoint at nlpaueb/legal-bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1508/1508 [55:11<00:00,  2.20s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1508/1508 [47:22<00:00,  1.88s/it]\n",
      "Some weights of the model checkpoint at anferico/bert-for-patents were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1508/1508 [2:32:46<00:00,  6.08s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1508/1508 [35:56<00:00,  1.43s/it]\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "for model_name in batch_2:\n",
    "\n",
    "    # supported masked language models (using bert)\n",
    "    if model_name in BERT_models:\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        model = BertForMaskedLM.from_pretrained(model_name)\n",
    "    elif model_name in ALBERT_models:\n",
    "        tokenizer = AlbertTokenizer.from_pretrained(model_name)\n",
    "        model = AlbertForMaskedLM.from_pretrained(model_name)\n",
    "    elif model_name in ROBERTA_models:\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "        model = RobertaForMaskedLM.from_pretrained(model_name)\n",
    "    elif model_name == 'xlm-roberta-base':\n",
    "        tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n",
    "        model = XLMRobertaForMaskedLM.from_pretrained(model_name)\n",
    "    elif model_name == 'distilbert-base-multilingual-cased':\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "        model = DistilBertForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # load data into panda DataFrame\n",
    "    df_data = read_data(\"crows_pairs_anonymized.csv\")\n",
    "\n",
    "    mask_token = tokenizer.mask_token\n",
    "    log_softmax = torch.nn.LogSoftmax(dim=0)\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    with open(\"bert\" + \".vocab\", \"w\") as f:\n",
    "        f.write(json.dumps(vocab))\n",
    "\n",
    "    lm = {\"model\": model,\n",
    "          \"tokenizer\": tokenizer,\n",
    "          \"mask_token\": mask_token,\n",
    "          \"log_softmax\": log_softmax,\n",
    "          \"uncased\": True\n",
    "    }\n",
    "\n",
    "    # score each sentence. \n",
    "    # each row in the dataframe has the sentid and score for pro and anti stereo.\n",
    "    df_score = pd.DataFrame(columns=['sent_more', 'sent_less', \n",
    "                                     'sent_more_score', 'sent_less_score',\n",
    "                                     'score', 'stereo_antistereo', 'bias_type'], dtype=object)\n",
    "\n",
    "    total = len(df_data.index)\n",
    "    with tqdm(total=total) as pbar:\n",
    "        for index, data in df_data.iterrows():\n",
    "            direction = data['direction']\n",
    "            bias = data['bias_type']\n",
    "            score = mask_unigram(data, lm)\n",
    "\n",
    "            for stype in score.keys():\n",
    "                score[stype] = round(score[stype], 3)\n",
    "\n",
    "            pair_score = 0\n",
    "            pbar.update(1)\n",
    "            if direction == 'stereo':\n",
    "                if score['sent1_score'] > score['sent2_score']:\n",
    "                    pair_score = 1\n",
    "            elif direction == 'antistereo':\n",
    "                if score['sent2_score'] > score['sent1_score']:\n",
    "                    pair_score = 1\n",
    "\n",
    "            sent_more, sent_less = '', ''\n",
    "            if direction == 'stereo':\n",
    "                sent_more = data['sent1']\n",
    "                sent_less = data['sent2']\n",
    "                sent_more_score = score['sent1_score']\n",
    "                sent_less_score = score['sent2_score']\n",
    "            else:\n",
    "                sent_more = data['sent2']\n",
    "                sent_less = data['sent1']\n",
    "                sent_more_score = score['sent2_score']\n",
    "                sent_less_score = score['sent1_score']\n",
    "\n",
    "            df_score = df_score.append({'sent_more': sent_more,\n",
    "                                        'sent_less': sent_less,\n",
    "                                        'sent_more_score': sent_more_score,\n",
    "                                        'sent_less_score': sent_less_score,\n",
    "                                        'score': pair_score,\n",
    "                                        'stereo_antistereo': direction,\n",
    "                                        'bias_type': bias\n",
    "                                      }, ignore_index=True)\n",
    "\n",
    "    file_path = 'All Output Files/Original CrowS-Pairs Dataset/' + model_name.split('/')[-1].lower() + '.csv'\n",
    "    df_score.to_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67e94421",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1508/1508 [42:07<00:00,  1.68s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1508/1508 [47:50<00:00,  1.90s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1508/1508 [52:34<00:00,  2.09s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1508/1508 [33:45<00:00,  1.34s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1508/1508 [2:39:04<00:00,  6.33s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1508/1508 [43:39<00:00,  1.74s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1508/1508 [33:14<00:00,  1.32s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1508/1508 [1:56:51<00:00,  4.65s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1508/1508 [58:44<00:00,  2.34s/it]\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "for model_name in batch_3:\n",
    "\n",
    "    # supported masked language models (using bert)\n",
    "    if model_name in BERT_models:\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        model = BertForMaskedLM.from_pretrained(model_name)\n",
    "    elif model_name in ALBERT_models:\n",
    "        tokenizer = AlbertTokenizer.from_pretrained(model_name)\n",
    "        model = AlbertForMaskedLM.from_pretrained(model_name)\n",
    "    elif model_name in ROBERTA_models:\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "        model = RobertaForMaskedLM.from_pretrained(model_name)\n",
    "    elif model_name == 'xlm-roberta-base':\n",
    "        tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n",
    "        model = XLMRobertaForMaskedLM.from_pretrained(model_name)\n",
    "    elif model_name == 'distilbert-base-multilingual-cased':\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "        model = DistilBertForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # load data into panda DataFrame\n",
    "    df_data = read_data(\"crows_pairs_anonymized.csv\")\n",
    "\n",
    "    mask_token = tokenizer.mask_token\n",
    "    log_softmax = torch.nn.LogSoftmax(dim=0)\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    with open(\"bert\" + \".vocab\", \"w\") as f:\n",
    "        f.write(json.dumps(vocab))\n",
    "\n",
    "    lm = {\"model\": model,\n",
    "          \"tokenizer\": tokenizer,\n",
    "          \"mask_token\": mask_token,\n",
    "          \"log_softmax\": log_softmax,\n",
    "          \"uncased\": True\n",
    "    }\n",
    "\n",
    "    # score each sentence. \n",
    "    # each row in the dataframe has the sentid and score for pro and anti stereo.\n",
    "    df_score = pd.DataFrame(columns=['sent_more', 'sent_less', \n",
    "                                     'sent_more_score', 'sent_less_score',\n",
    "                                     'score', 'stereo_antistereo', 'bias_type'], dtype=object)\n",
    "\n",
    "    total = len(df_data.index)\n",
    "    with tqdm(total=total) as pbar:\n",
    "        for index, data in df_data.iterrows():\n",
    "            direction = data['direction']\n",
    "            bias = data['bias_type']\n",
    "            score = mask_unigram(data, lm)\n",
    "\n",
    "            for stype in score.keys():\n",
    "                score[stype] = round(score[stype], 3)\n",
    "\n",
    "            pair_score = 0\n",
    "            pbar.update(1)\n",
    "            if direction == 'stereo':\n",
    "                if score['sent1_score'] > score['sent2_score']:\n",
    "                    pair_score = 1\n",
    "            elif direction == 'antistereo':\n",
    "                if score['sent2_score'] > score['sent1_score']:\n",
    "                    pair_score = 1\n",
    "\n",
    "            sent_more, sent_less = '', ''\n",
    "            if direction == 'stereo':\n",
    "                sent_more = data['sent1']\n",
    "                sent_less = data['sent2']\n",
    "                sent_more_score = score['sent1_score']\n",
    "                sent_less_score = score['sent2_score']\n",
    "            else:\n",
    "                sent_more = data['sent2']\n",
    "                sent_less = data['sent1']\n",
    "                sent_more_score = score['sent2_score']\n",
    "                sent_less_score = score['sent1_score']\n",
    "\n",
    "            df_score = df_score.append({'sent_more': sent_more,\n",
    "                                        'sent_less': sent_less,\n",
    "                                        'sent_more_score': sent_more_score,\n",
    "                                        'sent_less_score': sent_less_score,\n",
    "                                        'score': pair_score,\n",
    "                                        'stereo_antistereo': direction,\n",
    "                                        'bias_type': bias\n",
    "                                      }, ignore_index=True)\n",
    "\n",
    "    file_path = 'All Output Files/Original CrowS-Pairs Dataset/' + model_name.split('/')[-1].lower() + '.csv'\n",
    "    df_score.to_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bfde28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
