{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models_0 = pd.read_csv('social_bias_scores.csv', index_col=0)\n",
    "all_models_dataframes = [all_models_0]\n",
    "\n",
    "for threshold in ['1', '2-5', '5', '7-5', '10']:\n",
    "    \n",
    "    dataframe_variable_name = 'all_models_' + \"_\".join(threshold.split('-'))\n",
    "    file_name = 'social_bias_scores_threshold' + threshold + '.csv'\n",
    "    vars()[dataframe_variable_name] = pd.read_csv(file_name, index_col=0)\n",
    "    all_models_dataframes.append(vars()[dataframe_variable_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_model_names = ['social_bias_scores_threshold1.csv',\n",
    "                        'social_bias_scores_threshold2-5.csv',\n",
    "                        'social_bias_scores_threshold5.csv',\n",
    "                        'social_bias_scores_threshold7-5.csv',\n",
    "                        'social_bias_scores_threshold10.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    'bert-base-cased',\n",
    "    'bert-base-uncased',\n",
    "    'bert-large-uncased',\n",
    "    'bert-large-cased',\n",
    "    'bert-base-multilingual-uncased',\n",
    "    'bert-base-multilingual-cased',\n",
    "    'allenai/scibert_scivocab_uncased',\n",
    "    'emilyalsentzer/Bio_ClinicalBERT',\n",
    "    'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract',\n",
    "    'nlpaueb/legal-bert-base-uncased',\n",
    "    'GroNLP/hateBERT',\n",
    "    'anferico/bert-for-patents',\n",
    "    'jackaduma/SecBERT',\n",
    "    'albert-base-v1',\n",
    "    'albert-base-v2',\n",
    "    'roberta-base',\n",
    "    'distilroberta-base',\n",
    "    'roberta-large',\n",
    "    'huggingface/CodeBERTa-small-v1',\n",
    "    'climatebert/distilroberta-base-climate-f',\n",
    "    'xlm-roberta-base', \n",
    "    'distilbert-base-multilingual-cased']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_types = ['race-color',\n",
    "             'gender', \n",
    "             'socioeconomic',\n",
    "             'nationality', \n",
    "             'religion', \n",
    "             'age',\n",
    "             'sexual-orientation',\n",
    "             'physical-appearance',\n",
    "             'disability']\n",
    "\n",
    "bias_overall_weights = [516, 262, 172, \n",
    "               159, 105, 87, \n",
    "               84, 63, 60]\n",
    "\n",
    "bias_stereo_weights = [473, 159, 157, \n",
    "                      148, 99, 73, \n",
    "                      72, 52, 57] # 1290\n",
    "\n",
    "bias_antistereo_weights = [43, 103, 15, \n",
    "                          11, 6, 14, \n",
    "                          12, 11, 3] # 218"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_metrics_dict = {'model': [],\n",
    "                  'bias_type': [],\n",
    "                  'metric_score': [],\n",
    "                  'stereotype_score': [],\n",
    "                  'antistereotype_score': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for model_name in model_names:\n",
    "    \n",
    "    overall_metric = 0\n",
    "    overall_stereo = 0\n",
    "    overall_antistereo = 0\n",
    "    \n",
    "    model = all_models_0[all_models_0['model'] == model_name]\n",
    "    \n",
    "    for i in range(0,9):\n",
    "        \n",
    "        bias_type = bias_types[i]\n",
    "        bias_overall_weight = bias_overall_weights[i]\n",
    "        bias_stereo_weight = bias_stereo_weights[i]\n",
    "        bias_antistereo_weight = bias_antistereo_weights[i]\n",
    "        metric_score = float(model[model['bias_type']==bias_type]['metric_score'])\n",
    "        stereotype_score = float(model[model['bias_type']==bias_type]['stereotype_score'])\n",
    "        antistereotype_score = float(model[model['bias_type']==bias_type]['antistereotype_score'])\n",
    "        \n",
    "        overall_metrics_dict['model'].append(model_name)\n",
    "        overall_metrics_dict['bias_type'].append(bias_type)\n",
    "        overall_metrics_dict['metric_score'].append(metric_score)\n",
    "        overall_metrics_dict['stereotype_score'].append(stereotype_score)\n",
    "        overall_metrics_dict['antistereotype_score'].append(antistereotype_score)\n",
    "        \n",
    "        overall_metric += (metric_score * bias_overall_weight)\n",
    "        overall_stereo += (stereotype_score * bias_stereo_weight)\n",
    "        overall_antistereo += (antistereotype_score * bias_antistereo_weight)\n",
    "    \n",
    "    overall_metrics_dict['model'].append(model_name)\n",
    "    overall_metrics_dict['bias_type'].append('overall')\n",
    "    overall_metrics_dict['metric_score'].append(round(overall_metric / sum(bias_overall_weights), 2))\n",
    "    overall_metrics_dict['stereotype_score'].append(round(overall_stereo / sum(bias_stereo_weights), 2))\n",
    "    overall_metrics_dict['antistereotype_score'].append(round(overall_antistereo / sum(bias_antistereo_weights), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(overall_metrics_dict)\n",
    "output.to_csv('social_bias_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "for threshold_model_name in threshold_model_names:\n",
    "    \n",
    "    threshold_model = pd.read_csv(threshold_model_name, index_col=0)\n",
    "    \n",
    "    overall_metrics_dict = {'model': [],\n",
    "                            'bias_type': [],\n",
    "                            'metric_score': [],\n",
    "                            'stereotype_score': [],\n",
    "                            'antistereotype_score': [],\n",
    "                            'neutral_score': []}\n",
    "\n",
    "    for model_name in model_names:\n",
    "\n",
    "        overall_metric = 0\n",
    "        overall_stereo = 0\n",
    "        overall_antistereo = 0\n",
    "        overall_neutral = 0\n",
    "\n",
    "        model = threshold_model[threshold_model['model'] == model_name]\n",
    "\n",
    "        for i in range(0,9):\n",
    "\n",
    "            bias_type = bias_types[i]\n",
    "            bias_overall_weight = bias_overall_weights[i]\n",
    "            bias_stereo_weight = bias_stereo_weights[i]\n",
    "            bias_antistereo_weight = bias_antistereo_weights[i]\n",
    "            metric_score = float(model[model['bias_type']==bias_type]['metric_score'])\n",
    "            stereotype_score = float(model[model['bias_type']==bias_type]['stereotype_score'])\n",
    "            antistereotype_score = float(model[model['bias_type']==bias_type]['antistereotype_score'])\n",
    "            neutral_score = float(model[model['bias_type']==bias_type]['neutral_score'])\n",
    "\n",
    "            overall_metrics_dict['model'].append(model_name)\n",
    "            overall_metrics_dict['bias_type'].append(bias_type)\n",
    "            overall_metrics_dict['metric_score'].append(metric_score)\n",
    "            overall_metrics_dict['stereotype_score'].append(stereotype_score)\n",
    "            overall_metrics_dict['antistereotype_score'].append(antistereotype_score)\n",
    "            overall_metrics_dict['neutral_score'].append(neutral_score)\n",
    "\n",
    "            overall_metric += (metric_score * bias_overall_weight)\n",
    "            overall_stereo += (stereotype_score * bias_stereo_weight)\n",
    "            overall_antistereo += (antistereotype_score * bias_antistereo_weight)\n",
    "            overall_neutral += (neutral_score * bias_overall_weight)\n",
    "\n",
    "        overall_metrics_dict['model'].append(model_name)\n",
    "        overall_metrics_dict['bias_type'].append('overall')\n",
    "        overall_metrics_dict['metric_score'].append(round(overall_metric / sum(bias_overall_weights), 2))\n",
    "        overall_metrics_dict['stereotype_score'].append(round(overall_stereo / sum(bias_stereo_weights), 2))\n",
    "        overall_metrics_dict['antistereotype_score'].append(round(overall_antistereo / sum(bias_antistereo_weights), 2))\n",
    "        overall_metrics_dict['neutral_score'].append(round(overall_metric / sum(bias_overall_weights), 2))\n",
    "    \n",
    "    output = pd.DataFrame(overall_metrics_dict)\n",
    "    output.to_csv(threshold_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
