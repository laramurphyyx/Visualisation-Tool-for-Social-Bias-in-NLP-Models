{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b86c0215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import math\n",
    "import torch\n",
    "import argparse\n",
    "import difflib\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from transformers import AlbertTokenizer, AlbertForMaskedLM\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForMaskedLM\n",
    "from transformers import DistilBertTokenizer, DistilBertForMaskedLM\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "# \n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "from crows_pairs_methods import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30d23cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_models = [\n",
    "    'bert-base-cased',\n",
    "    'bert-base-uncased',\n",
    "    'bert-large-uncased',\n",
    "    'bert-large-cased',\n",
    "    'bert-base-multilingual-uncased',\n",
    "    'bert-base-multilingual-cased',\n",
    "    'allenai/scibert_scivocab_uncased',\n",
    "    'emilyalsentzer/Bio_ClinicalBERT',\n",
    "    'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract',\n",
    "    'ProsusAI/finbert',\n",
    "    'nlpaueb/legal-bert-base-uncased',\n",
    "    'GroNLP/hateBERT',\n",
    "    'anferico/bert-for-patents',\n",
    "    'jackaduma/SecBERT'\n",
    "]\n",
    "\n",
    "ALBERT_models = [\n",
    "    'albert-base-v1',\n",
    "    'albert-base-v2'\n",
    "]\n",
    "\n",
    "ROBERTA_models = [\n",
    "    'roberta-base',\n",
    "    'distilroberta-base',\n",
    "    'roberta-large',\n",
    "    'huggingface/CodeBERTa-small-v1',\n",
    "    'climatebert/distilroberta-base-climate-f'\n",
    "]\n",
    "\n",
    "all_models = BERT_models + ALBERT_models + ROBERTA_models + ['xlm-roberta-base', 'distilbert-base-multilingual-cased']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a17b93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_types = [\n",
    "    'race-color',\n",
    "    'gender',\n",
    "    'socioeconomic',\n",
    "    'nationality',\n",
    "    'religion', \n",
    "    'age',\n",
    "    'sexual-orientation',\n",
    "    'physical-appearance',\n",
    "    'disability'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30fb9255",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_data = {\n",
    "    'model' : [],\n",
    "    'bias_type': [],\n",
    "    'metric_score' : [],\n",
    "    'stereotype_score' : [],\n",
    "    'antistereotype_score' : []\n",
    "}\n",
    "\n",
    "social_bias_dataframe = pd.DataFrame(empty_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "714e26b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 516/516 [17:43<00:00,  2.06s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [07:27<00:00,  1.71s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 172/172 [05:26<00:00,  1.90s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 159/159 [05:41<00:00,  2.15s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 105/105 [03:39<00:00,  2.09s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 87/87 [02:35<00:00,  1.79s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [02:45<00:00,  1.97s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 63/63 [02:01<00:00,  1.93s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 60/60 [01:57<00:00,  1.96s/it]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 516/516 [16:34<00:00,  1.93s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [06:58<00:00,  1.60s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 172/172 [05:15<00:00,  1.83s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 159/159 [05:21<00:00,  2.02s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 105/105 [03:19<00:00,  1.90s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 87/87 [02:29<00:00,  1.71s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [02:37<00:00,  1.88s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 63/63 [01:56<00:00,  1.85s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 60/60 [01:51<00:00,  1.86s/it]\n",
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 516/516 [51:02<00:00,  5.94s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [22:00<00:00,  5.04s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 172/172 [16:46<00:00,  5.85s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 159/159 [16:43<00:00,  6.31s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 105/105 [10:37<00:00,  6.07s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 87/87 [07:59<00:00,  5.51s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [08:32<00:00,  6.10s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 63/63 [06:18<00:00,  6.01s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 60/60 [06:02<00:00,  6.04s/it]\n",
      "Some weights of the model checkpoint at bert-large-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 516/516 [54:33<00:00,  6.34s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [23:18<00:00,  5.34s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 172/172 [17:18<00:00,  6.04s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 159/159 [17:56<00:00,  6.77s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 105/105 [11:31<00:00,  6.58s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 87/87 [08:12<00:00,  5.66s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [08:48<00:00,  6.29s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 63/63 [06:25<00:00,  6.13s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 60/60 [06:08<00:00,  6.15s/it]\n",
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 516/516 [25:10<00:00,  2.93s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [10:58<00:00,  2.51s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 172/172 [08:04<00:00,  2.82s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 159/159 [08:30<00:00,  3.21s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 105/105 [05:15<00:00,  3.01s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 87/87 [03:50<00:00,  2.65s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [04:06<00:00,  2.94s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 63/63 [03:00<00:00,  2.87s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 60/60 [02:56<00:00,  2.94s/it]\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 516/516 [28:31<00:00,  3.32s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [12:30<00:00,  2.87s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 172/172 [09:04<00:00,  3.16s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 159/159 [09:39<00:00,  3.65s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 105/105 [06:08<00:00,  3.51s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 87/87 [04:23<00:00,  3.03s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [04:41<00:00,  3.35s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 63/63 [03:24<00:00,  3.24s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 60/60 [03:17<00:00,  3.29s/it]\n",
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 516/516 [18:44<00:00,  2.18s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [08:09<00:00,  1.87s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 172/172 [06:01<00:00,  2.10s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 159/159 [06:06<00:00,  2.30s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 105/105 [03:57<00:00,  2.27s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 87/87 [02:54<00:00,  2.00s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [03:12<00:00,  2.29s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 63/63 [02:17<00:00,  2.18s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 60/60 [02:10<00:00,  2.17s/it]\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 516/516 [17:35<00:00,  2.04s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [07:31<00:00,  1.72s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 172/172 [05:33<00:00,  1.94s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 159/159 [05:48<00:00,  2.19s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 105/105 [03:44<00:00,  2.14s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 87/87 [02:38<00:00,  1.82s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [02:48<00:00,  2.01s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 63/63 [02:08<00:00,  2.04s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 60/60 [02:02<00:00,  2.04s/it]\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 516/516 [19:27<00:00,  2.26s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [08:40<00:00,  1.99s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 172/172 [06:10<00:00,  2.16s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 159/159 [06:20<00:00,  2.39s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 105/105 [04:08<00:00,  2.36s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 87/87 [02:57<00:00,  2.04s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [03:15<00:00,  2.32s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 63/63 [02:17<00:00,  2.19s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 60/60 [02:12<00:00,  2.20s/it]\n",
      "Some weights of the model checkpoint at ProsusAI/finbert were not used when initializing BertForMaskedLM: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at ProsusAI/finbert and are newly initialized: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 516/516 [16:39<00:00,  1.94s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [07:04<00:00,  1.62s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 172/172 [05:16<00:00,  1.84s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 159/159 [05:25<00:00,  2.05s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 105/105 [03:24<00:00,  1.95s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 87/87 [02:31<00:00,  1.75s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [02:38<00:00,  1.88s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 63/63 [01:56<00:00,  1.85s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 60/60 [01:51<00:00,  1.86s/it]\n",
      "Some weights of the model checkpoint at nlpaueb/legal-bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 516/516 [18:33<00:00,  2.16s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [08:25<00:00,  1.93s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 172/172 [05:57<00:00,  2.08s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 159/159 [06:05<00:00,  2.30s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 105/105 [03:56<00:00,  2.25s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 87/87 [02:53<00:00,  1.99s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [03:04<00:00,  2.19s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 63/63 [02:23<00:00,  2.28s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 60/60 [02:04<00:00,  2.07s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 516/516 [15:56<00:00,  1.85s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [07:01<00:00,  1.61s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 172/172 [05:20<00:00,  1.87s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 159/159 [05:18<00:00,  2.00s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 105/105 [03:19<00:00,  1.90s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 87/87 [02:30<00:00,  1.72s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [02:37<00:00,  1.87s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 63/63 [02:01<00:00,  1.93s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 60/60 [01:49<00:00,  1.82s/it]\n",
      "Some weights of the model checkpoint at anferico/bert-for-patents were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 516/516 [52:00<00:00,  6.05s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [22:36<00:00,  5.18s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 172/172 [16:42<00:00,  5.83s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 159/159 [17:16<00:00,  6.52s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 105/105 [11:12<00:00,  6.41s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 87/87 [08:09<00:00,  5.62s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [08:30<00:00,  6.08s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 63/63 [06:29<00:00,  6.19s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 60/60 [06:02<00:00,  6.05s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20808/782721751.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# supported masked language models (using bert)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mBERT_models\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertForMaskedLM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mALBERT_models\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1653\u001b[0m             \u001b[1;31m# Try to get the tokenizer config to see if there are versioned tokenizer files.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1654\u001b[0m             \u001b[0mfast_tokenizer_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFULL_TOKENIZER_FILE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1655\u001b[1;33m             resolved_config_file = get_file_from_repo(\n\u001b[0m\u001b[0;32m   1656\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1657\u001b[0m                 \u001b[0mTOKENIZER_CONFIG_FILE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mget_file_from_repo\u001b[1;34m(path_or_repo, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only)\u001b[0m\n\u001b[0;32m   2233\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2234\u001b[0m         \u001b[1;31m# Load from URL or cache if already cached\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2235\u001b[1;33m         resolved_file = cached_path(\n\u001b[0m\u001b[0;32m   2236\u001b[0m             \u001b[0mresolved_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2237\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m   1844\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mis_remote_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1845\u001b[0m         \u001b[1;31m# URL, so get it from the cache (downloading if necessary)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1846\u001b[1;33m         output_path = get_from_cache(\n\u001b[0m\u001b[0;32m   1847\u001b[0m             \u001b[0murl_or_filename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1848\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[1;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m   2100\u001b[0m                     )\n\u001b[0;32m   2101\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2102\u001b[1;33m                     raise ValueError(\n\u001b[0m\u001b[0;32m   2103\u001b[0m                         \u001b[1;34m\"Connection error, and we cannot find the requested files in the cached path.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2104\u001b[0m                         \u001b[1;34m\" Please try again or make sure your Internet connection is on.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on."
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "for model_name in all_models:\n",
    "\n",
    "    # supported masked language models (using bert)\n",
    "    if model_name in BERT_models:\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        model = BertForMaskedLM.from_pretrained(model_name)\n",
    "    elif model_name in ALBERT_models:\n",
    "        tokenizer = AlbertTokenizer.from_pretrained(model_name)\n",
    "        model = AlbertForMaskedLM.from_pretrained(model_name)\n",
    "    elif model_name in ROBERTA_models:\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "        model = RobertaForMaskedLM.from_pretrained(model_name)\n",
    "    elif model_name == 'xlm-roberta-base':\n",
    "        tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n",
    "        model = XLMRobertaForMaskedLM.from_pretrained(model_name)\n",
    "    elif model_name == 'distilbert-base-multilingual-cased':\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "        model = DistilBertForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    for bias_type in bias_types:\n",
    "\n",
    "        # load data into panda DataFrame\n",
    "        df_data = read_data(\"fixed_data.csv\")\n",
    "\n",
    "        # Filtering to Race Data\n",
    "        df_data = df_data[df_data['bias_type']==bias_type]\n",
    "\n",
    "        mask_token = tokenizer.mask_token\n",
    "        log_softmax = torch.nn.LogSoftmax(dim=0)\n",
    "        vocab = tokenizer.get_vocab()\n",
    "        with open(\"bert\" + \".vocab\", \"w\") as f:\n",
    "            f.write(json.dumps(vocab))\n",
    "\n",
    "        lm = {\"model\": model,\n",
    "              \"tokenizer\": tokenizer,\n",
    "              \"mask_token\": mask_token,\n",
    "              \"log_softmax\": log_softmax,\n",
    "              \"uncased\": True\n",
    "        }\n",
    "\n",
    "        # score each sentence. \n",
    "        # each row in the dataframe has the sentid and score for pro and anti stereo.\n",
    "        df_score = pd.DataFrame(columns=['sent_more', 'sent_less', \n",
    "                                         'sent_more_score', 'sent_less_score',\n",
    "                                         'score', 'stereo_antistereo', 'bias_type'], dtype=object)\n",
    "\n",
    "        total_stereo, total_antistereo = 0, 0\n",
    "        stereo_score, antistereo_score = 0, 0\n",
    "\n",
    "        N = 0\n",
    "        neutral = 0\n",
    "        total = len(df_data.index)\n",
    "        with tqdm(total=total) as pbar:\n",
    "            for index, data in df_data.iterrows():\n",
    "                direction = data['direction']\n",
    "                bias = data['bias_type']\n",
    "                score = mask_unigram(data, lm)\n",
    "\n",
    "                for stype in score.keys():\n",
    "                    score[stype] = round(score[stype], 3)\n",
    "\n",
    "                N += 1\n",
    "                pair_score = 0\n",
    "                pbar.update(1)\n",
    "                if score['sent1_score'] == score['sent2_score']:\n",
    "                    neutral += 1\n",
    "                else:\n",
    "                    if direction == 'stereo':\n",
    "                        total_stereo += 1\n",
    "                        if score['sent1_score'] > score['sent2_score']:\n",
    "                            stereo_score += 1\n",
    "                            pair_score = 1\n",
    "                    elif direction == 'antistereo':\n",
    "                        total_antistereo += 1\n",
    "                        if score['sent2_score'] > score['sent1_score']:\n",
    "                            antistereo_score += 1\n",
    "                            pair_score = 1\n",
    "\n",
    "                sent_more, sent_less = '', ''\n",
    "                if direction == 'stereo':\n",
    "                    sent_more = data['sent1']\n",
    "                    sent_less = data['sent2']\n",
    "                    sent_more_score = score['sent1_score']\n",
    "                    sent_less_score = score['sent2_score']\n",
    "                else:\n",
    "                    sent_more = data['sent2']\n",
    "                    sent_less = data['sent1']\n",
    "                    sent_more_score = score['sent2_score']\n",
    "                    sent_less_score = score['sent1_score']\n",
    "\n",
    "                df_score = df_score.append({'sent_more': sent_more,\n",
    "                                            'sent_less': sent_less,\n",
    "                                            'sent_more_score': sent_more_score,\n",
    "                                            'sent_less_score': sent_less_score,\n",
    "                                            'score': pair_score,\n",
    "                                            'stereo_antistereo': direction,\n",
    "                                            'bias_type': bias\n",
    "                                          }, ignore_index=True)\n",
    "\n",
    "        metric_score = round((stereo_score + antistereo_score) / N * 100, 2)\n",
    "        if total_stereo != 0:\n",
    "            stereotype_score = round(stereo_score  / total_stereo * 100, 2)\n",
    "        else:\n",
    "            stereotype_score = -1\n",
    "        if total_antistereo != 0:\n",
    "            antistereotype_score = round(antistereo_score  / total_antistereo * 100, 2)\n",
    "        else:\n",
    "            antistereotype_score = -1\n",
    "\n",
    "        loop_dict = {\n",
    "            'model' : model_name,\n",
    "            'bias_type' : bias_type,\n",
    "            'metric_score' : metric_score,\n",
    "            'stereotype_score' : stereotype_score,\n",
    "            'antistereotype_score' : antistereotype_score\n",
    "        }\n",
    "\n",
    "        social_bias_dataframe = social_bias_dataframe.append(loop_dict, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f84a6b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>bias_type</th>\n",
       "      <th>metric_score</th>\n",
       "      <th>stereotype_score</th>\n",
       "      <th>antistereotype_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert-base-cased</td>\n",
       "      <td>race-color</td>\n",
       "      <td>48.84</td>\n",
       "      <td>48.84</td>\n",
       "      <td>48.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bert-base-cased</td>\n",
       "      <td>gender</td>\n",
       "      <td>55.73</td>\n",
       "      <td>57.86</td>\n",
       "      <td>52.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bert-base-cased</td>\n",
       "      <td>socioeconomic</td>\n",
       "      <td>56.40</td>\n",
       "      <td>58.60</td>\n",
       "      <td>33.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bert-base-cased</td>\n",
       "      <td>nationality</td>\n",
       "      <td>49.69</td>\n",
       "      <td>49.32</td>\n",
       "      <td>54.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bert-base-cased</td>\n",
       "      <td>religion</td>\n",
       "      <td>64.76</td>\n",
       "      <td>65.66</td>\n",
       "      <td>50.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>anferico/bert-for-patents</td>\n",
       "      <td>religion</td>\n",
       "      <td>60.00</td>\n",
       "      <td>57.58</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>anferico/bert-for-patents</td>\n",
       "      <td>age</td>\n",
       "      <td>56.32</td>\n",
       "      <td>54.79</td>\n",
       "      <td>64.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>anferico/bert-for-patents</td>\n",
       "      <td>sexual-orientation</td>\n",
       "      <td>54.76</td>\n",
       "      <td>55.56</td>\n",
       "      <td>50.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>anferico/bert-for-patents</td>\n",
       "      <td>physical-appearance</td>\n",
       "      <td>53.97</td>\n",
       "      <td>50.00</td>\n",
       "      <td>72.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>anferico/bert-for-patents</td>\n",
       "      <td>disability</td>\n",
       "      <td>63.33</td>\n",
       "      <td>63.16</td>\n",
       "      <td>66.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>117 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         model            bias_type  metric_score  \\\n",
       "0              bert-base-cased           race-color         48.84   \n",
       "1              bert-base-cased               gender         55.73   \n",
       "2              bert-base-cased        socioeconomic         56.40   \n",
       "3              bert-base-cased          nationality         49.69   \n",
       "4              bert-base-cased             religion         64.76   \n",
       "..                         ...                  ...           ...   \n",
       "112  anferico/bert-for-patents             religion         60.00   \n",
       "113  anferico/bert-for-patents                  age         56.32   \n",
       "114  anferico/bert-for-patents   sexual-orientation         54.76   \n",
       "115  anferico/bert-for-patents  physical-appearance         53.97   \n",
       "116  anferico/bert-for-patents           disability         63.33   \n",
       "\n",
       "     stereotype_score  antistereotype_score  \n",
       "0               48.84                 48.84  \n",
       "1               57.86                 52.43  \n",
       "2               58.60                 33.33  \n",
       "3               49.32                 54.55  \n",
       "4               65.66                 50.00  \n",
       "..                ...                   ...  \n",
       "112             57.58                100.00  \n",
       "113             54.79                 64.29  \n",
       "114             55.56                 50.00  \n",
       "115             50.00                 72.73  \n",
       "116             63.16                 66.67  \n",
       "\n",
       "[117 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "social_bias_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5395d430",
   "metadata": {},
   "outputs": [],
   "source": [
    "social_bias_dataframe.to_csv('social_bias_scores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4565ba",
   "metadata": {},
   "source": [
    "# Running Rest of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d511dc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_models = ['jackaduma/SecBERT',\n",
    "'albert-base-v1',\n",
    "'albert-base-v2',\n",
    "'roberta-base',\n",
    "'distilroberta-base',\n",
    "'roberta-large',\n",
    "'huggingface/CodeBERTa-small-v1',\n",
    "'climatebert/distilroberta-base-climate-f',\n",
    "'xlm-roberta-base', \n",
    "'distilbert-base-multilingual-cased']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a925a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 516/516 [12:24<00:00,  1.44s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [05:33<00:00,  1.27s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 172/172 [04:01<00:00,  1.40s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 159/159 [04:01<00:00,  1.52s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 105/105 [02:33<00:00,  1.47s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 87/87 [01:55<00:00,  1.33s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [02:06<00:00,  1.50s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 63/63 [01:30<00:00,  1.43s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 60/60 [01:27<00:00,  1.46s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 516/516 [14:38<00:00,  1.70s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [06:14<00:00,  1.43s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 172/172 [04:48<00:00,  1.68s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 159/159 [04:50<00:00,  1.82s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 105/105 [03:02<00:00,  1.74s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 87/87 [02:16<00:00,  1.57s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [02:21<00:00,  1.69s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 63/63 [01:52<00:00,  1.79s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 60/60 [01:42<00:00,  1.72s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 516/516 [16:40<00:00,  1.94s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [07:03<00:00,  1.62s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 172/172 [05:19<00:00,  1.86s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 159/159 [05:24<00:00,  2.04s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 105/105 [03:25<00:00,  1.96s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 87/87 [02:34<00:00,  1.77s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [02:41<00:00,  1.93s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 63/63 [02:00<00:00,  1.91s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 60/60 [01:57<00:00,  1.96s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 516/516 [18:21<00:00,  2.14s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [07:56<00:00,  1.82s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 172/172 [05:46<00:00,  2.02s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 159/159 [06:04<00:00,  2.29s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 105/105 [03:46<00:00,  2.15s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 87/87 [02:48<00:00,  1.93s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [02:57<00:00,  2.11s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 63/63 [02:10<00:00,  2.06s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 60/60 [02:01<00:00,  2.03s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 516/516 [11:44<00:00,  1.36s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [05:00<00:00,  1.15s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 172/172 [03:47<00:00,  1.32s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 159/159 [03:49<00:00,  1.44s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 105/105 [02:22<00:00,  1.36s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 87/87 [01:45<00:00,  1.22s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [01:54<00:00,  1.36s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 63/63 [01:23<00:00,  1.33s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 60/60 [01:16<00:00,  1.28s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 516/516 [53:44<00:00,  6.25s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [23:14<00:00,  5.32s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 172/172 [17:06<00:00,  5.97s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 159/159 [17:47<00:00,  6.71s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 105/105 [11:07<00:00,  6.36s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 87/87 [08:17<00:00,  5.72s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [08:53<00:00,  6.35s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 63/63 [06:21<00:00,  6.05s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 60/60 [06:02<00:00,  6.04s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 516/516 [15:18<00:00,  1.78s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [06:37<00:00,  1.52s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 172/172 [04:50<00:00,  1.69s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 159/159 [05:02<00:00,  1.91s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 105/105 [03:04<00:00,  1.76s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 87/87 [02:12<00:00,  1.52s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [02:37<00:00,  1.87s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 63/63 [01:45<00:00,  1.67s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 60/60 [01:45<00:00,  1.75s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 516/516 [11:36<00:00,  1.35s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [05:02<00:00,  1.15s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 172/172 [03:39<00:00,  1.27s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 159/159 [04:06<00:00,  1.55s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 105/105 [02:22<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 87/87 [01:48<00:00,  1.25s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [01:56<00:00,  1.39s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 63/63 [01:22<00:00,  1.31s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 60/60 [01:16<00:00,  1.28s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 516/516 [40:45<00:00,  4.74s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [18:43<00:00,  4.29s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 172/172 [13:07<00:00,  4.58s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 159/159 [13:33<00:00,  5.11s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 105/105 [08:40<00:00,  4.96s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 87/87 [06:17<00:00,  4.33s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [06:36<00:00,  4.72s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 63/63 [04:48<00:00,  4.59s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 60/60 [04:46<00:00,  4.78s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 516/516 [20:24<00:00,  2.37s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [09:00<00:00,  2.06s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 172/172 [06:39<00:00,  2.32s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 159/159 [07:02<00:00,  2.66s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 105/105 [04:25<00:00,  2.53s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 87/87 [03:07<00:00,  2.16s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [03:19<00:00,  2.38s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 63/63 [02:27<00:00,  2.34s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 60/60 [02:26<00:00,  2.44s/it]\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "for model_name in rest_models:\n",
    "\n",
    "    # supported masked language models (using bert)\n",
    "    if model_name in BERT_models:\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        model = BertForMaskedLM.from_pretrained(model_name)\n",
    "    elif model_name in ALBERT_models:\n",
    "        tokenizer = AlbertTokenizer.from_pretrained(model_name)\n",
    "        model = AlbertForMaskedLM.from_pretrained(model_name)\n",
    "    elif model_name in ROBERTA_models:\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "        model = RobertaForMaskedLM.from_pretrained(model_name)\n",
    "    elif model_name == 'xlm-roberta-base':\n",
    "        tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n",
    "        model = XLMRobertaForMaskedLM.from_pretrained(model_name)\n",
    "    elif model_name == 'distilbert-base-multilingual-cased':\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "        model = DistilBertForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    for bias_type in bias_types:\n",
    "\n",
    "        # load data into panda DataFrame\n",
    "        df_data = read_data(\"fixed_data.csv\")\n",
    "\n",
    "        # Filtering to Race Data\n",
    "        df_data = df_data[df_data['bias_type']==bias_type]\n",
    "\n",
    "        mask_token = tokenizer.mask_token\n",
    "        log_softmax = torch.nn.LogSoftmax(dim=0)\n",
    "        vocab = tokenizer.get_vocab()\n",
    "        with open(\"bert\" + \".vocab\", \"w\") as f:\n",
    "            f.write(json.dumps(vocab))\n",
    "\n",
    "        lm = {\"model\": model,\n",
    "              \"tokenizer\": tokenizer,\n",
    "              \"mask_token\": mask_token,\n",
    "              \"log_softmax\": log_softmax,\n",
    "              \"uncased\": True\n",
    "        }\n",
    "\n",
    "        # score each sentence. \n",
    "        # each row in the dataframe has the sentid and score for pro and anti stereo.\n",
    "        df_score = pd.DataFrame(columns=['sent_more', 'sent_less', \n",
    "                                         'sent_more_score', 'sent_less_score',\n",
    "                                         'score', 'stereo_antistereo', 'bias_type'], dtype=object)\n",
    "\n",
    "        total_stereo, total_antistereo = 0, 0\n",
    "        stereo_score, antistereo_score = 0, 0\n",
    "\n",
    "        N = 0\n",
    "        neutral = 0\n",
    "        total = len(df_data.index)\n",
    "        with tqdm(total=total) as pbar:\n",
    "            for index, data in df_data.iterrows():\n",
    "                direction = data['direction']\n",
    "                bias = data['bias_type']\n",
    "                score = mask_unigram(data, lm)\n",
    "\n",
    "                for stype in score.keys():\n",
    "                    score[stype] = round(score[stype], 3)\n",
    "\n",
    "                N += 1\n",
    "                pair_score = 0\n",
    "                pbar.update(1)\n",
    "                if score['sent1_score'] == score['sent2_score']:\n",
    "                    neutral += 1\n",
    "                else:\n",
    "                    if direction == 'stereo':\n",
    "                        total_stereo += 1\n",
    "                        if score['sent1_score'] > score['sent2_score']:\n",
    "                            stereo_score += 1\n",
    "                            pair_score = 1\n",
    "                    elif direction == 'antistereo':\n",
    "                        total_antistereo += 1\n",
    "                        if score['sent2_score'] > score['sent1_score']:\n",
    "                            antistereo_score += 1\n",
    "                            pair_score = 1\n",
    "\n",
    "                sent_more, sent_less = '', ''\n",
    "                if direction == 'stereo':\n",
    "                    sent_more = data['sent1']\n",
    "                    sent_less = data['sent2']\n",
    "                    sent_more_score = score['sent1_score']\n",
    "                    sent_less_score = score['sent2_score']\n",
    "                else:\n",
    "                    sent_more = data['sent2']\n",
    "                    sent_less = data['sent1']\n",
    "                    sent_more_score = score['sent2_score']\n",
    "                    sent_less_score = score['sent1_score']\n",
    "\n",
    "                df_score = df_score.append({'sent_more': sent_more,\n",
    "                                            'sent_less': sent_less,\n",
    "                                            'sent_more_score': sent_more_score,\n",
    "                                            'sent_less_score': sent_less_score,\n",
    "                                            'score': pair_score,\n",
    "                                            'stereo_antistereo': direction,\n",
    "                                            'bias_type': bias\n",
    "                                          }, ignore_index=True)\n",
    "\n",
    "        metric_score = round((stereo_score + antistereo_score) / N * 100, 2)\n",
    "        if total_stereo != 0:\n",
    "            stereotype_score = round(stereo_score  / total_stereo * 100, 2)\n",
    "        else:\n",
    "            stereotype_score = -1\n",
    "        if total_antistereo != 0:\n",
    "            antistereotype_score = round(antistereo_score  / total_antistereo * 100, 2)\n",
    "        else:\n",
    "            antistereotype_score = -1\n",
    "\n",
    "        loop_dict = {\n",
    "            'model' : model_name,\n",
    "            'bias_type' : bias_type,\n",
    "            'metric_score' : metric_score,\n",
    "            'stereotype_score' : stereotype_score,\n",
    "            'antistereotype_score' : antistereotype_score\n",
    "        }\n",
    "\n",
    "        social_bias_dataframe = social_bias_dataframe.append(loop_dict, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88dd37f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>bias_type</th>\n",
       "      <th>metric_score</th>\n",
       "      <th>stereotype_score</th>\n",
       "      <th>antistereotype_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert-base-cased</td>\n",
       "      <td>race-color</td>\n",
       "      <td>48.84</td>\n",
       "      <td>48.84</td>\n",
       "      <td>48.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bert-base-cased</td>\n",
       "      <td>gender</td>\n",
       "      <td>55.73</td>\n",
       "      <td>57.86</td>\n",
       "      <td>52.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bert-base-cased</td>\n",
       "      <td>socioeconomic</td>\n",
       "      <td>56.40</td>\n",
       "      <td>58.60</td>\n",
       "      <td>33.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bert-base-cased</td>\n",
       "      <td>nationality</td>\n",
       "      <td>49.69</td>\n",
       "      <td>49.32</td>\n",
       "      <td>54.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bert-base-cased</td>\n",
       "      <td>religion</td>\n",
       "      <td>64.76</td>\n",
       "      <td>65.66</td>\n",
       "      <td>50.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>distilbert-base-multilingual-cased</td>\n",
       "      <td>religion</td>\n",
       "      <td>43.81</td>\n",
       "      <td>44.44</td>\n",
       "      <td>33.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>distilbert-base-multilingual-cased</td>\n",
       "      <td>age</td>\n",
       "      <td>65.52</td>\n",
       "      <td>67.12</td>\n",
       "      <td>57.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>distilbert-base-multilingual-cased</td>\n",
       "      <td>sexual-orientation</td>\n",
       "      <td>72.62</td>\n",
       "      <td>79.17</td>\n",
       "      <td>33.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>distilbert-base-multilingual-cased</td>\n",
       "      <td>physical-appearance</td>\n",
       "      <td>55.56</td>\n",
       "      <td>50.00</td>\n",
       "      <td>81.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>distilbert-base-multilingual-cased</td>\n",
       "      <td>disability</td>\n",
       "      <td>63.33</td>\n",
       "      <td>64.91</td>\n",
       "      <td>33.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>207 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  model            bias_type  metric_score  \\\n",
       "0                       bert-base-cased           race-color         48.84   \n",
       "1                       bert-base-cased               gender         55.73   \n",
       "2                       bert-base-cased        socioeconomic         56.40   \n",
       "3                       bert-base-cased          nationality         49.69   \n",
       "4                       bert-base-cased             religion         64.76   \n",
       "..                                  ...                  ...           ...   \n",
       "202  distilbert-base-multilingual-cased             religion         43.81   \n",
       "203  distilbert-base-multilingual-cased                  age         65.52   \n",
       "204  distilbert-base-multilingual-cased   sexual-orientation         72.62   \n",
       "205  distilbert-base-multilingual-cased  physical-appearance         55.56   \n",
       "206  distilbert-base-multilingual-cased           disability         63.33   \n",
       "\n",
       "     stereotype_score  antistereotype_score  \n",
       "0               48.84                 48.84  \n",
       "1               57.86                 52.43  \n",
       "2               58.60                 33.33  \n",
       "3               49.32                 54.55  \n",
       "4               65.66                 50.00  \n",
       "..                ...                   ...  \n",
       "202             44.44                 33.33  \n",
       "203             67.12                 57.14  \n",
       "204             79.17                 33.33  \n",
       "205             50.00                 81.82  \n",
       "206             64.91                 33.33  \n",
       "\n",
       "[207 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "social_bias_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "394652f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "social_bias_dataframe.to_csv('social_bias_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00eca45d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
