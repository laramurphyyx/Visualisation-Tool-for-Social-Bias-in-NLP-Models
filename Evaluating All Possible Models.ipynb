{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import math\n",
    "import torch\n",
    "import argparse\n",
    "import difflib\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from transformers import AlbertTokenizer, AlbertForMaskedLM\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForMaskedLM\n",
    "from transformers import DistilBertTokenizer, DistilBertForMaskedLM\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "# \n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crows_pairs_methods import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of models and their tokenizers that are running on the below tests are:\n",
    "\n",
    "* <b>'bert-large-uncased'</b>: Using BertTokenizer and BertForMaskedLM\n",
    "* <b>'albert-base-v2'</b>: Using AlbertTokenizer and AlbertForMaskedLM \n",
    "* <b>'distilroberta-base'</b>: Using RobertaTokenizer and RobertaForMaskedLM\n",
    "* <b>'allenai/scibert_scivocab_uncased'</b>: Using BertTokenizer and BertForMaskedLM\n",
    "* <b>'bert-base-multilingual-uncased'</b>: Using BertTokenizer and BertForMaskedLM\n",
    "* <b>'bert-base-multilingual-cased'</b>: Using BertTokenizer and BertForMaskedLM\n",
    "* <b>'roberta-base'</b>: Using RobertaTokenizer and RobertaForMaskedLM\n",
    "* <b>'bert-base-cased'</b>: Using BertTokenizer and BertForMaskedLM\n",
    "* <b>'xlm-roberta-base'</b>: Using XLMRobertaTokenizer, XLMRobertaForMaskedLM # multilingual roberta\n",
    "* <b>'roberta-large'</b>: Using RobertaTokenizer and RobertaForMaskedLM\n",
    "* <b>'distilbert-base-multilingual-cased'</b>: Using BertTokenizer and BertForMaskedLM\n",
    "* <b>'emilyalsentzer/Bio_ClinicalBERT'</b>: Using BertTokenizer and BertForMaskedLM\n",
    "* <b>'huggingface/CodeBERTa-small-v1'</b>: Using RobertaTokenizer and RobertaForMaskedLM\n",
    "* <b>'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract'</b>: Using BertTokenizer and BertForMaskedLM\n",
    "* <b>'ProsusAI/finbert'</b>: Using BertTokenizer and BertForMaskedLM\n",
    "* <b>'nlpaueb/legal-bert-base-uncased'</b>: Using BertTokenizer and BertForMaskedLM\n",
    "* <b>'vinai/bertweet-base'</b>: # doesnt work\n",
    "* <b>'GroNLP/hateBERT'</b>: Using BertTokenizer and BertForMaskedLM #### very interesting possibly\n",
    "* <b>'anferico/bert-for-patents'</b>: Using BertTokenizer and BertForMaskedLM ## (seems large)\n",
    "* <b>'climatebert/distilroberta-base-climate-f'</b>: Using RobertaTokenizer and RobertaForMaskedLM\n",
    "* <b>'jackaduma/SecBERT'</b>: Using BertTokenizer and BertForMaskedLM\n",
    "\n",
    "Not tested are:\n",
    "* <b>'xlm-roberta-large'</b>\n",
    "* <b>'bert-large-cased'</b>\n",
    "* <b>'facebook/muppet-roberta-large'</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_models = [\n",
    "    'bert-base-cased',\n",
    "    'bert-base-uncased',\n",
    "    'bert-large-uncased',\n",
    "    'bert-large-cased',\n",
    "    'bert-base-multilingual-uncased',\n",
    "    'bert-base-multilingual-cased',\n",
    "    'allenai/scibert_scivocab_uncased',\n",
    "    'emilyalsentzer/Bio_ClinicalBERT',\n",
    "    'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract',\n",
    "    'ProsusAI/finbert',\n",
    "    'nlpaueb/legal-bert-base-uncased',\n",
    "    'GroNLP/hateBERT',\n",
    "    'anferico/bert-for-patents',\n",
    "    'jackaduma/SecBERT'\n",
    "]\n",
    "\n",
    "ALBERT_models = [\n",
    "    'albert-base-v1',\n",
    "    'albert-base-v2'\n",
    "]\n",
    "\n",
    "ROBERTA_models = [\n",
    "    'roberta-base',\n",
    "    'distilroberta-base',\n",
    "    'roberta-large',\n",
    "    'huggingface/CodeBERTa-small-v1',\n",
    "    'climatebert/distilroberta-base-climate-f'\n",
    "]\n",
    "\n",
    "all_models = BERT_models + ALBERT_models + ROBERTA_models + ['xlm-roberta-base', 'distilbert-base-multilingual-cased']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_dictionary = {'models' : [],\n",
    "'metric_scores' : [],\n",
    "'stereotype_scores' : [],\n",
    "'antistereotype_scores' : []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:13<00:00,  2.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            models  metric_scores  stereotype_scores  antistereotype_scores\n",
      "0  bert-base-cased          100.0              100.0                  100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:13<00:00,  2.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              models  metric_scores  stereotype_scores  antistereotype_scores\n",
      "0    bert-base-cased          100.0              100.0                  100.0\n",
      "1  bert-base-uncased           60.0               75.0                    0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:39<00:00,  8.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               models  metric_scores  stereotype_scores  antistereotype_scores\n",
      "0     bert-base-cased          100.0              100.0                  100.0\n",
      "1   bert-base-uncased           60.0               75.0                    0.0\n",
      "2  bert-large-uncased           80.0               75.0                  100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:38<00:00,  7.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               models  metric_scores  stereotype_scores  antistereotype_scores\n",
      "0     bert-base-cased          100.0              100.0                  100.0\n",
      "1   bert-base-uncased           60.0               75.0                    0.0\n",
      "2  bert-large-uncased           80.0               75.0                  100.0\n",
      "3    bert-large-cased           80.0              100.0                    0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:17<00:00,  3.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           models  metric_scores  stereotype_scores  \\\n",
      "0                 bert-base-cased          100.0              100.0   \n",
      "1               bert-base-uncased           60.0               75.0   \n",
      "2              bert-large-uncased           80.0               75.0   \n",
      "3                bert-large-cased           80.0              100.0   \n",
      "4  bert-base-multilingual-uncased           80.0               75.0   \n",
      "\n",
      "   antistereotype_scores  \n",
      "0                  100.0  \n",
      "1                    0.0  \n",
      "2                  100.0  \n",
      "3                    0.0  \n",
      "4                  100.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:19<00:00,  3.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           models  metric_scores  stereotype_scores  \\\n",
      "0                 bert-base-cased          100.0              100.0   \n",
      "1               bert-base-uncased           60.0               75.0   \n",
      "2              bert-large-uncased           80.0               75.0   \n",
      "3                bert-large-cased           80.0              100.0   \n",
      "4  bert-base-multilingual-uncased           80.0               75.0   \n",
      "5    bert-base-multilingual-cased           60.0               75.0   \n",
      "\n",
      "   antistereotype_scores  \n",
      "0                  100.0  \n",
      "1                    0.0  \n",
      "2                  100.0  \n",
      "3                    0.0  \n",
      "4                  100.0  \n",
      "5                    0.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at distilbert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'vocab_projector.weight', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'vocab_transform.bias', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'vocab_transform.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'vocab_projector.bias', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.3.attention.out_lin.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['encoder.layer.7.intermediate.dense.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.1.attention.self.value.bias', 'cls.predictions.transform.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.bias', 'cls.predictions.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.6.attention.output.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.5.output.dense.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.9.attention.self.key.bias', 'cls.predictions.transform.LayerNorm.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.output.dense.bias', 'cls.predictions.decoder.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:19<00:00,  3.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               models  metric_scores  stereotype_scores  \\\n",
      "0                     bert-base-cased          100.0              100.0   \n",
      "1                   bert-base-uncased           60.0               75.0   \n",
      "2                  bert-large-uncased           80.0               75.0   \n",
      "3                    bert-large-cased           80.0              100.0   \n",
      "4      bert-base-multilingual-uncased           80.0               75.0   \n",
      "5        bert-base-multilingual-cased           60.0               75.0   \n",
      "6  distilbert-base-multilingual-cased           40.0               25.0   \n",
      "\n",
      "   antistereotype_scores  \n",
      "0                  100.0  \n",
      "1                    0.0  \n",
      "2                  100.0  \n",
      "3                    0.0  \n",
      "4                  100.0  \n",
      "5                    0.0  \n",
      "6                  100.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:11<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               models  metric_scores  stereotype_scores  \\\n",
      "0                     bert-base-cased          100.0              100.0   \n",
      "1                   bert-base-uncased           60.0               75.0   \n",
      "2                  bert-large-uncased           80.0               75.0   \n",
      "3                    bert-large-cased           80.0              100.0   \n",
      "4      bert-base-multilingual-uncased           80.0               75.0   \n",
      "5        bert-base-multilingual-cased           60.0               75.0   \n",
      "6  distilbert-base-multilingual-cased           40.0               25.0   \n",
      "7    allenai/scibert_scivocab_uncased           40.0               50.0   \n",
      "\n",
      "   antistereotype_scores  \n",
      "0                  100.0  \n",
      "1                    0.0  \n",
      "2                  100.0  \n",
      "3                    0.0  \n",
      "4                  100.0  \n",
      "5                    0.0  \n",
      "6                  100.0  \n",
      "7                    0.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:11<00:00,  2.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               models  metric_scores  stereotype_scores  \\\n",
      "0                     bert-base-cased          100.0              100.0   \n",
      "1                   bert-base-uncased           60.0               75.0   \n",
      "2                  bert-large-uncased           80.0               75.0   \n",
      "3                    bert-large-cased           80.0              100.0   \n",
      "4      bert-base-multilingual-uncased           80.0               75.0   \n",
      "5        bert-base-multilingual-cased           60.0               75.0   \n",
      "6  distilbert-base-multilingual-cased           40.0               25.0   \n",
      "7    allenai/scibert_scivocab_uncased           40.0               50.0   \n",
      "8     emilyalsentzer/Bio_ClinicalBERT           40.0               25.0   \n",
      "\n",
      "   antistereotype_scores  \n",
      "0                  100.0  \n",
      "1                    0.0  \n",
      "2                  100.0  \n",
      "3                    0.0  \n",
      "4                  100.0  \n",
      "5                    0.0  \n",
      "6                  100.0  \n",
      "7                    0.0  \n",
      "8                  100.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:11<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              models  metric_scores  \\\n",
      "0                                    bert-base-cased          100.0   \n",
      "1                                  bert-base-uncased           60.0   \n",
      "2                                 bert-large-uncased           80.0   \n",
      "3                                   bert-large-cased           80.0   \n",
      "4                     bert-base-multilingual-uncased           80.0   \n",
      "5                       bert-base-multilingual-cased           60.0   \n",
      "6                 distilbert-base-multilingual-cased           40.0   \n",
      "7                   allenai/scibert_scivocab_uncased           40.0   \n",
      "8                    emilyalsentzer/Bio_ClinicalBERT           40.0   \n",
      "9  microsoft/BiomedNLP-PubMedBERT-base-uncased-ab...           60.0   \n",
      "\n",
      "   stereotype_scores  antistereotype_scores  \n",
      "0              100.0                  100.0  \n",
      "1               75.0                    0.0  \n",
      "2               75.0                  100.0  \n",
      "3              100.0                    0.0  \n",
      "4               75.0                  100.0  \n",
      "5               75.0                    0.0  \n",
      "6               25.0                  100.0  \n",
      "7               50.0                    0.0  \n",
      "8               25.0                  100.0  \n",
      "9               75.0                    0.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ProsusAI/finbert were not used when initializing BertForMaskedLM: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at ProsusAI/finbert and are newly initialized: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:11<00:00,  2.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               models  metric_scores  \\\n",
      "0                                     bert-base-cased          100.0   \n",
      "1                                   bert-base-uncased           60.0   \n",
      "2                                  bert-large-uncased           80.0   \n",
      "3                                    bert-large-cased           80.0   \n",
      "4                      bert-base-multilingual-uncased           80.0   \n",
      "5                        bert-base-multilingual-cased           60.0   \n",
      "6                  distilbert-base-multilingual-cased           40.0   \n",
      "7                    allenai/scibert_scivocab_uncased           40.0   \n",
      "8                     emilyalsentzer/Bio_ClinicalBERT           40.0   \n",
      "9   microsoft/BiomedNLP-PubMedBERT-base-uncased-ab...           60.0   \n",
      "10                                   ProsusAI/finbert           60.0   \n",
      "\n",
      "    stereotype_scores  antistereotype_scores  \n",
      "0               100.0                  100.0  \n",
      "1                75.0                    0.0  \n",
      "2                75.0                  100.0  \n",
      "3               100.0                    0.0  \n",
      "4                75.0                  100.0  \n",
      "5                75.0                    0.0  \n",
      "6                25.0                  100.0  \n",
      "7                50.0                    0.0  \n",
      "8                25.0                  100.0  \n",
      "9                75.0                    0.0  \n",
      "10               50.0                  100.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nlpaueb/legal-bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:12<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               models  metric_scores  \\\n",
      "0                                     bert-base-cased          100.0   \n",
      "1                                   bert-base-uncased           60.0   \n",
      "2                                  bert-large-uncased           80.0   \n",
      "3                                    bert-large-cased           80.0   \n",
      "4                      bert-base-multilingual-uncased           80.0   \n",
      "5                        bert-base-multilingual-cased           60.0   \n",
      "6                  distilbert-base-multilingual-cased           40.0   \n",
      "7                    allenai/scibert_scivocab_uncased           40.0   \n",
      "8                     emilyalsentzer/Bio_ClinicalBERT           40.0   \n",
      "9   microsoft/BiomedNLP-PubMedBERT-base-uncased-ab...           60.0   \n",
      "10                                   ProsusAI/finbert           60.0   \n",
      "11                    nlpaueb/legal-bert-base-uncased          100.0   \n",
      "\n",
      "    stereotype_scores  antistereotype_scores  \n",
      "0               100.0                  100.0  \n",
      "1                75.0                    0.0  \n",
      "2                75.0                  100.0  \n",
      "3               100.0                    0.0  \n",
      "4                75.0                  100.0  \n",
      "5                75.0                    0.0  \n",
      "6                25.0                  100.0  \n",
      "7                50.0                    0.0  \n",
      "8                25.0                  100.0  \n",
      "9                75.0                    0.0  \n",
      "10               50.0                  100.0  \n",
      "11              100.0                  100.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:12<00:00,  2.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               models  metric_scores  \\\n",
      "0                                     bert-base-cased          100.0   \n",
      "1                                   bert-base-uncased           60.0   \n",
      "2                                  bert-large-uncased           80.0   \n",
      "3                                    bert-large-cased           80.0   \n",
      "4                      bert-base-multilingual-uncased           80.0   \n",
      "5                        bert-base-multilingual-cased           60.0   \n",
      "6                  distilbert-base-multilingual-cased           40.0   \n",
      "7                    allenai/scibert_scivocab_uncased           40.0   \n",
      "8                     emilyalsentzer/Bio_ClinicalBERT           40.0   \n",
      "9   microsoft/BiomedNLP-PubMedBERT-base-uncased-ab...           60.0   \n",
      "10                                   ProsusAI/finbert           60.0   \n",
      "11                    nlpaueb/legal-bert-base-uncased          100.0   \n",
      "12                                    GroNLP/hateBERT           60.0   \n",
      "\n",
      "    stereotype_scores  antistereotype_scores  \n",
      "0               100.0                  100.0  \n",
      "1                75.0                    0.0  \n",
      "2                75.0                  100.0  \n",
      "3               100.0                    0.0  \n",
      "4                75.0                  100.0  \n",
      "5                75.0                    0.0  \n",
      "6                25.0                  100.0  \n",
      "7                50.0                    0.0  \n",
      "8                25.0                  100.0  \n",
      "9                75.0                    0.0  \n",
      "10               50.0                  100.0  \n",
      "11              100.0                  100.0  \n",
      "12               75.0                    0.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at anferico/bert-for-patents were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:41<00:00,  8.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               models  metric_scores  \\\n",
      "0                                     bert-base-cased          100.0   \n",
      "1                                   bert-base-uncased           60.0   \n",
      "2                                  bert-large-uncased           80.0   \n",
      "3                                    bert-large-cased           80.0   \n",
      "4                      bert-base-multilingual-uncased           80.0   \n",
      "5                        bert-base-multilingual-cased           60.0   \n",
      "6                  distilbert-base-multilingual-cased           40.0   \n",
      "7                    allenai/scibert_scivocab_uncased           40.0   \n",
      "8                     emilyalsentzer/Bio_ClinicalBERT           40.0   \n",
      "9   microsoft/BiomedNLP-PubMedBERT-base-uncased-ab...           60.0   \n",
      "10                                   ProsusAI/finbert           60.0   \n",
      "11                    nlpaueb/legal-bert-base-uncased          100.0   \n",
      "12                                    GroNLP/hateBERT           60.0   \n",
      "13                          anferico/bert-for-patents           60.0   \n",
      "\n",
      "    stereotype_scores  antistereotype_scores  \n",
      "0               100.0                  100.0  \n",
      "1                75.0                    0.0  \n",
      "2                75.0                  100.0  \n",
      "3               100.0                    0.0  \n",
      "4                75.0                  100.0  \n",
      "5                75.0                    0.0  \n",
      "6                25.0                  100.0  \n",
      "7                50.0                    0.0  \n",
      "8                25.0                  100.0  \n",
      "9                75.0                    0.0  \n",
      "10               50.0                  100.0  \n",
      "11              100.0                  100.0  \n",
      "12               75.0                    0.0  \n",
      "13               50.0                  100.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:07<00:00,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               models  metric_scores  \\\n",
      "0                                     bert-base-cased          100.0   \n",
      "1                                   bert-base-uncased           60.0   \n",
      "2                                  bert-large-uncased           80.0   \n",
      "3                                    bert-large-cased           80.0   \n",
      "4                      bert-base-multilingual-uncased           80.0   \n",
      "5                        bert-base-multilingual-cased           60.0   \n",
      "6                  distilbert-base-multilingual-cased           40.0   \n",
      "7                    allenai/scibert_scivocab_uncased           40.0   \n",
      "8                     emilyalsentzer/Bio_ClinicalBERT           40.0   \n",
      "9   microsoft/BiomedNLP-PubMedBERT-base-uncased-ab...           60.0   \n",
      "10                                   ProsusAI/finbert           60.0   \n",
      "11                    nlpaueb/legal-bert-base-uncased          100.0   \n",
      "12                                    GroNLP/hateBERT           60.0   \n",
      "13                          anferico/bert-for-patents           60.0   \n",
      "14                                  jackaduma/SecBERT           60.0   \n",
      "\n",
      "    stereotype_scores  antistereotype_scores  \n",
      "0               100.0                  100.0  \n",
      "1                75.0                    0.0  \n",
      "2                75.0                  100.0  \n",
      "3               100.0                    0.0  \n",
      "4                75.0                  100.0  \n",
      "5                75.0                    0.0  \n",
      "6                25.0                  100.0  \n",
      "7                50.0                    0.0  \n",
      "8                25.0                  100.0  \n",
      "9                75.0                    0.0  \n",
      "10               50.0                  100.0  \n",
      "11              100.0                  100.0  \n",
      "12               75.0                    0.0  \n",
      "13               50.0                  100.0  \n",
      "14               50.0                  100.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:09<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               models  metric_scores  \\\n",
      "0                                     bert-base-cased          100.0   \n",
      "1                                   bert-base-uncased           60.0   \n",
      "2                                  bert-large-uncased           80.0   \n",
      "3                                    bert-large-cased           80.0   \n",
      "4                      bert-base-multilingual-uncased           80.0   \n",
      "5                        bert-base-multilingual-cased           60.0   \n",
      "6                  distilbert-base-multilingual-cased           40.0   \n",
      "7                    allenai/scibert_scivocab_uncased           40.0   \n",
      "8                     emilyalsentzer/Bio_ClinicalBERT           40.0   \n",
      "9   microsoft/BiomedNLP-PubMedBERT-base-uncased-ab...           60.0   \n",
      "10                                   ProsusAI/finbert           60.0   \n",
      "11                    nlpaueb/legal-bert-base-uncased          100.0   \n",
      "12                                    GroNLP/hateBERT           60.0   \n",
      "13                          anferico/bert-for-patents           60.0   \n",
      "14                                  jackaduma/SecBERT           60.0   \n",
      "15                                     albert-base-v1           60.0   \n",
      "\n",
      "    stereotype_scores  antistereotype_scores  \n",
      "0               100.0                  100.0  \n",
      "1                75.0                    0.0  \n",
      "2                75.0                  100.0  \n",
      "3               100.0                    0.0  \n",
      "4                75.0                  100.0  \n",
      "5                75.0                    0.0  \n",
      "6                25.0                  100.0  \n",
      "7                50.0                    0.0  \n",
      "8                25.0                  100.0  \n",
      "9                75.0                    0.0  \n",
      "10               50.0                  100.0  \n",
      "11              100.0                  100.0  \n",
      "12               75.0                    0.0  \n",
      "13               50.0                  100.0  \n",
      "14               50.0                  100.0  \n",
      "15               75.0                    0.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:09<00:00,  1.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               models  metric_scores  \\\n",
      "0                                     bert-base-cased          100.0   \n",
      "1                                   bert-base-uncased           60.0   \n",
      "2                                  bert-large-uncased           80.0   \n",
      "3                                    bert-large-cased           80.0   \n",
      "4                      bert-base-multilingual-uncased           80.0   \n",
      "5                        bert-base-multilingual-cased           60.0   \n",
      "6                  distilbert-base-multilingual-cased           40.0   \n",
      "7                    allenai/scibert_scivocab_uncased           40.0   \n",
      "8                     emilyalsentzer/Bio_ClinicalBERT           40.0   \n",
      "9   microsoft/BiomedNLP-PubMedBERT-base-uncased-ab...           60.0   \n",
      "10                                   ProsusAI/finbert           60.0   \n",
      "11                    nlpaueb/legal-bert-base-uncased          100.0   \n",
      "12                                    GroNLP/hateBERT           60.0   \n",
      "13                          anferico/bert-for-patents           60.0   \n",
      "14                                  jackaduma/SecBERT           60.0   \n",
      "15                                     albert-base-v1           60.0   \n",
      "16                                     albert-base-v2          100.0   \n",
      "\n",
      "    stereotype_scores  antistereotype_scores  \n",
      "0               100.0                  100.0  \n",
      "1                75.0                    0.0  \n",
      "2                75.0                  100.0  \n",
      "3               100.0                    0.0  \n",
      "4                75.0                  100.0  \n",
      "5                75.0                    0.0  \n",
      "6                25.0                  100.0  \n",
      "7                50.0                    0.0  \n",
      "8                25.0                  100.0  \n",
      "9                75.0                    0.0  \n",
      "10               50.0                  100.0  \n",
      "11              100.0                  100.0  \n",
      "12               75.0                    0.0  \n",
      "13               50.0                  100.0  \n",
      "14               50.0                  100.0  \n",
      "15               75.0                    0.0  \n",
      "16              100.0                  100.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:15<00:00,  3.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               models  metric_scores  \\\n",
      "0                                     bert-base-cased          100.0   \n",
      "1                                   bert-base-uncased           60.0   \n",
      "2                                  bert-large-uncased           80.0   \n",
      "3                                    bert-large-cased           80.0   \n",
      "4                      bert-base-multilingual-uncased           80.0   \n",
      "5                        bert-base-multilingual-cased           60.0   \n",
      "6                  distilbert-base-multilingual-cased           40.0   \n",
      "7                    allenai/scibert_scivocab_uncased           40.0   \n",
      "8                     emilyalsentzer/Bio_ClinicalBERT           40.0   \n",
      "9   microsoft/BiomedNLP-PubMedBERT-base-uncased-ab...           60.0   \n",
      "10                                   ProsusAI/finbert           60.0   \n",
      "11                    nlpaueb/legal-bert-base-uncased          100.0   \n",
      "12                                    GroNLP/hateBERT           60.0   \n",
      "13                          anferico/bert-for-patents           60.0   \n",
      "14                                  jackaduma/SecBERT           60.0   \n",
      "15                                     albert-base-v1           60.0   \n",
      "16                                     albert-base-v2          100.0   \n",
      "17                                       roberta-base          100.0   \n",
      "\n",
      "    stereotype_scores  antistereotype_scores  \n",
      "0               100.0                  100.0  \n",
      "1                75.0                    0.0  \n",
      "2                75.0                  100.0  \n",
      "3               100.0                    0.0  \n",
      "4                75.0                  100.0  \n",
      "5                75.0                    0.0  \n",
      "6                25.0                  100.0  \n",
      "7                50.0                    0.0  \n",
      "8                25.0                  100.0  \n",
      "9                75.0                    0.0  \n",
      "10               50.0                  100.0  \n",
      "11              100.0                  100.0  \n",
      "12               75.0                    0.0  \n",
      "13               50.0                  100.0  \n",
      "14               50.0                  100.0  \n",
      "15               75.0                    0.0  \n",
      "16              100.0                  100.0  \n",
      "17              100.0                  100.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:09<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               models  metric_scores  \\\n",
      "0                                     bert-base-cased          100.0   \n",
      "1                                   bert-base-uncased           60.0   \n",
      "2                                  bert-large-uncased           80.0   \n",
      "3                                    bert-large-cased           80.0   \n",
      "4                      bert-base-multilingual-uncased           80.0   \n",
      "5                        bert-base-multilingual-cased           60.0   \n",
      "6                  distilbert-base-multilingual-cased           40.0   \n",
      "7                    allenai/scibert_scivocab_uncased           40.0   \n",
      "8                     emilyalsentzer/Bio_ClinicalBERT           40.0   \n",
      "9   microsoft/BiomedNLP-PubMedBERT-base-uncased-ab...           60.0   \n",
      "10                                   ProsusAI/finbert           60.0   \n",
      "11                    nlpaueb/legal-bert-base-uncased          100.0   \n",
      "12                                    GroNLP/hateBERT           60.0   \n",
      "13                          anferico/bert-for-patents           60.0   \n",
      "14                                  jackaduma/SecBERT           60.0   \n",
      "15                                     albert-base-v1           60.0   \n",
      "16                                     albert-base-v2          100.0   \n",
      "17                                       roberta-base          100.0   \n",
      "18                                 distilroberta-base           80.0   \n",
      "\n",
      "    stereotype_scores  antistereotype_scores  \n",
      "0               100.0                  100.0  \n",
      "1                75.0                    0.0  \n",
      "2                75.0                  100.0  \n",
      "3               100.0                    0.0  \n",
      "4                75.0                  100.0  \n",
      "5                75.0                    0.0  \n",
      "6                25.0                  100.0  \n",
      "7                50.0                    0.0  \n",
      "8                25.0                  100.0  \n",
      "9                75.0                    0.0  \n",
      "10               50.0                  100.0  \n",
      "11              100.0                  100.0  \n",
      "12               75.0                    0.0  \n",
      "13               50.0                  100.0  \n",
      "14               50.0                  100.0  \n",
      "15               75.0                    0.0  \n",
      "16              100.0                  100.0  \n",
      "17              100.0                  100.0  \n",
      "18               75.0                  100.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:42<00:00,  8.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               models  metric_scores  \\\n",
      "0                                     bert-base-cased          100.0   \n",
      "1                                   bert-base-uncased           60.0   \n",
      "2                                  bert-large-uncased           80.0   \n",
      "3                                    bert-large-cased           80.0   \n",
      "4                      bert-base-multilingual-uncased           80.0   \n",
      "5                        bert-base-multilingual-cased           60.0   \n",
      "6                  distilbert-base-multilingual-cased           40.0   \n",
      "7                    allenai/scibert_scivocab_uncased           40.0   \n",
      "8                     emilyalsentzer/Bio_ClinicalBERT           40.0   \n",
      "9   microsoft/BiomedNLP-PubMedBERT-base-uncased-ab...           60.0   \n",
      "10                                   ProsusAI/finbert           60.0   \n",
      "11                    nlpaueb/legal-bert-base-uncased          100.0   \n",
      "12                                    GroNLP/hateBERT           60.0   \n",
      "13                          anferico/bert-for-patents           60.0   \n",
      "14                                  jackaduma/SecBERT           60.0   \n",
      "15                                     albert-base-v1           60.0   \n",
      "16                                     albert-base-v2          100.0   \n",
      "17                                       roberta-base          100.0   \n",
      "18                                 distilroberta-base           80.0   \n",
      "19                                      roberta-large           80.0   \n",
      "\n",
      "    stereotype_scores  antistereotype_scores  \n",
      "0               100.0                  100.0  \n",
      "1                75.0                    0.0  \n",
      "2                75.0                  100.0  \n",
      "3               100.0                    0.0  \n",
      "4                75.0                  100.0  \n",
      "5                75.0                    0.0  \n",
      "6                25.0                  100.0  \n",
      "7                50.0                    0.0  \n",
      "8                25.0                  100.0  \n",
      "9                75.0                    0.0  \n",
      "10               50.0                  100.0  \n",
      "11              100.0                  100.0  \n",
      "12               75.0                    0.0  \n",
      "13               50.0                  100.0  \n",
      "14               50.0                  100.0  \n",
      "15               75.0                    0.0  \n",
      "16              100.0                  100.0  \n",
      "17              100.0                  100.0  \n",
      "18               75.0                  100.0  \n",
      "19               75.0                  100.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLMRobertaTokenizer'. \n",
      "The class this function is called from is 'RobertaTokenizer'.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-91-d8989ab1bdf9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAlbertForMaskedLM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mROBERTA_models\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRobertaTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRobertaForMaskedLM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1748\u001b[0m             \u001b[1;33m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1749\u001b[0m             \u001b[0muse_auth_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1750\u001b[1;33m             \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1751\u001b[0m         )\n\u001b[0;32m   1752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1870\u001b[0m         \u001b[1;31m# Instantiate tokenizer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1871\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1872\u001b[1;33m             \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0minit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1873\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1874\u001b[0m             raise OSError(\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\tokenization_roberta.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, vocab_file, merges_file, errors, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, add_prefix_space, **kwargs)\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[0mmask_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask_token\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m             \u001b[0madd_prefix_space\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0madd_prefix_space\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m             \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m         )\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\transformers\\models\\gpt2\\tokenization_gpt2.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, vocab_file, merges_file, errors, unk_token, bos_token, eos_token, add_prefix_space, **kwargs)\u001b[0m\n\u001b[0;32m    177\u001b[0m         )\n\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mvocab_handle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_handle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "for model_name in all_models:\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    # load data into panda DataFrame\n",
    "    df_data = read_data(\"crows_pairs_anonymized.csv\")\n",
    "\n",
    "    # Filtering to Disability Data\n",
    "    df_data = df_data[df_data['bias_type']=='disability'][:5]\n",
    "    \n",
    "    # supported masked language models (using bert)\n",
    "    if model_name in BERT_models:\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        model = BertForMaskedLM.from_pretrained(model_name)\n",
    "    elif model_name in ALBERT_models:\n",
    "        tokenizer = AlbertTokenizer.from_pretrained(model_name)\n",
    "        model = AlbertForMaskedLM.from_pretrained(model_name)\n",
    "    elif model_name in ROBERTA_models:\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "        model = RobertaForMaskedLM.from_pretrained(model_name)\n",
    "    elif model_name == 'xlm-roberta-base':\n",
    "        tokenizer = XMLRobertaTokenizer.from_pretrained(model_name)\n",
    "        model = XMLRobertaForMaskedLM.from_pretrained(model_name)\n",
    "    elif model_name == 'distilbert-base-multilingual-cased':\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "        model = DistilBertForMaskedLM.from_pretrained(model_name)\n",
    "        \n",
    "    mask_token = tokenizer.mask_token\n",
    "    log_softmax = torch.nn.LogSoftmax(dim=0)\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    with open(\"bert\" + \".vocab\", \"w\") as f:\n",
    "        f.write(json.dumps(vocab))\n",
    "\n",
    "    lm = {\"model\": model,\n",
    "          \"tokenizer\": tokenizer,\n",
    "          \"mask_token\": mask_token,\n",
    "          \"log_softmax\": log_softmax,\n",
    "          \"uncased\": True\n",
    "    }\n",
    "\n",
    "    # score each sentence. \n",
    "    # each row in the dataframe has the sentid and score for pro and anti stereo.\n",
    "    df_score = pd.DataFrame(columns=['sent_more', 'sent_less', \n",
    "                                     'sent_more_score', 'sent_less_score',\n",
    "                                     'score', 'stereo_antistereo', 'bias_type'], dtype=object)\n",
    "    \n",
    "    total_stereo, total_antistereo = 0, 0\n",
    "    stereo_score, antistereo_score = 0, 0\n",
    "\n",
    "    N = 0\n",
    "    neutral = 0\n",
    "    total = len(df_data.index)\n",
    "    with tqdm(total=total) as pbar:\n",
    "        for index, data in df_data.iterrows():\n",
    "            direction = data['direction']\n",
    "            bias = data['bias_type']\n",
    "            score = mask_unigram(data, lm)\n",
    "\n",
    "            for stype in score.keys():\n",
    "                score[stype] = round(score[stype], 3)\n",
    "\n",
    "            N += 1\n",
    "            pair_score = 0\n",
    "            pbar.update(1)\n",
    "            if score['sent1_score'] == score['sent2_score']:\n",
    "                neutral += 1\n",
    "            else:\n",
    "                if direction == 'stereo':\n",
    "                    total_stereo += 1\n",
    "                    if score['sent1_score'] > score['sent2_score']:\n",
    "                        stereo_score += 1\n",
    "                        pair_score = 1\n",
    "                elif direction == 'antistereo':\n",
    "                    total_antistereo += 1\n",
    "                    if score['sent2_score'] > score['sent1_score']:\n",
    "                        antistereo_score += 1\n",
    "                        pair_score = 1\n",
    "\n",
    "            sent_more, sent_less = '', ''\n",
    "            if direction == 'stereo':\n",
    "                sent_more = data['sent1']\n",
    "                sent_less = data['sent2']\n",
    "                sent_more_score = score['sent1_score']\n",
    "                sent_less_score = score['sent2_score']\n",
    "            else:\n",
    "                sent_more = data['sent2']\n",
    "                sent_less = data['sent1']\n",
    "                sent_more_score = score['sent2_score']\n",
    "                sent_less_score = score['sent1_score']\n",
    "\n",
    "            df_score = df_score.append({'sent_more': sent_more,\n",
    "                                        'sent_less': sent_less,\n",
    "                                        'sent_more_score': sent_more_score,\n",
    "                                        'sent_less_score': sent_less_score,\n",
    "                                        'score': pair_score,\n",
    "                                        'stereo_antistereo': direction,\n",
    "                                        'bias_type': bias\n",
    "                                      }, ignore_index=True)\n",
    "    \n",
    "    dataframe_dictionary['models'].append(model_name)\n",
    "    dataframe_dictionary['metric_scores'].append(round((stereo_score + antistereo_score) / N * 100, 2))\n",
    "    dataframe_dictionary['stereotype_scores'].append(round(stereo_score  / total_stereo * 100, 2))\n",
    "    dataframe_dictionary['antistereotype_scores'].append(round(antistereo_score  / total_antistereo * 100, 2))\n",
    "    \n",
    "    print(pd.DataFrame(dataframe_dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20 language models were tested (cancelled on XMLRoberta, so it's 20 out of 24)\n",
    "\n",
    "Timings for 5 sentences (and estimated duration of running entire set) on each model:\n",
    "* 42s (3.5 hours) - roberta large\n",
    "* 9s - (45 mins) distil roberta base\n",
    "* 15s - (1.25 hours) roberta base\n",
    "* 9s - (45 mins) albert base v2\n",
    "* 9s - (45 mins) albert base v1\n",
    "* 7s - (35 mins) security bert\n",
    "* 41s - (3.4 hours) patents\n",
    "* 12s - (1 hour) hate bert\n",
    "* 12s - (1 hour) legal\n",
    "* 11s - (55 mins) finbert\n",
    "* 11s - (55 mins) biomed pubmed base uncased abstract\n",
    "* 11s - (55 mins) bio clinical bert\n",
    "* 11s - (55 mins) scibert scivocab uncased\n",
    "* 19s - (1.6 hours) distil bert base multilingual cased\n",
    "* 19s - (1.6 hours) bert base multilingual cased\n",
    "* 17s - (1.4 hours) bert base multilingual uncased\n",
    "* 38s - (3.2 hours) bert large cased\n",
    "* 39s - (3.3 hours) bert large uncased\n",
    "* 13s - (1 hour) bert base uncased\n",
    "* 13s - (1 hour) bert base cased\n",
    "\n",
    "The sum of all of these are approximately 31 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Models on Gender-Biased Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_dataframe_dictionary = {'models' : [],\n",
    "'metric_scores' : [],\n",
    "'stereotype_scores' : [],\n",
    "'antistereotype_scores' : []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [10:56<00:00,  2.50s/it]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [11:12<00:00,  2.57s/it]\n",
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [36:06<00:00,  8.27s/it]\n",
      "Some weights of the model checkpoint at bert-large-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [37:20<00:00,  8.55s/it]\n",
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [17:39<00:00,  4.04s/it]\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [18:55<00:00,  4.34s/it]\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at distilbert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'vocab_projector.weight', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'vocab_transform.bias', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'vocab_transform.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'vocab_projector.bias', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.3.attention.out_lin.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['encoder.layer.7.intermediate.dense.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.1.attention.self.value.bias', 'cls.predictions.transform.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.bias', 'cls.predictions.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.6.attention.output.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.5.output.dense.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.9.attention.self.key.bias', 'cls.predictions.transform.LayerNorm.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.output.dense.bias', 'cls.predictions.decoder.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [18:56<00:00,  4.34s/it]\n",
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [12:19<00:00,  2.82s/it]\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [11:25<00:00,  2.62s/it]\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [12:40<00:00,  2.90s/it]\n",
      "Some weights of the model checkpoint at ProsusAI/finbert were not used when initializing BertForMaskedLM: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at ProsusAI/finbert and are newly initialized: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [11:13<00:00,  2.57s/it]\n",
      "Some weights of the model checkpoint at nlpaueb/legal-bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [12:34<00:00,  2.88s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [11:16<00:00,  2.58s/it]\n",
      "Some weights of the model checkpoint at anferico/bert-for-patents were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [38:43<00:00,  8.87s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [09:04<00:00,  2.08s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [09:54<00:00,  2.27s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [10:54<00:00,  2.50s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [13:44<00:00,  3.15s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [08:39<00:00,  1.98s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [40:19<00:00,  9.23s/it]\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLMRobertaTokenizer'. \n",
      "The class this function is called from is 'RobertaTokenizer'.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-99-cb439835a39c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAlbertForMaskedLM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mROBERTA_models\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRobertaTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRobertaForMaskedLM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'xlm-roberta-base'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1748\u001b[0m             \u001b[1;33m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1749\u001b[0m             \u001b[0muse_auth_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1750\u001b[1;33m             \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1751\u001b[0m         )\n\u001b[0;32m   1752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1870\u001b[0m         \u001b[1;31m# Instantiate tokenizer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1871\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1872\u001b[1;33m             \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0minit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1873\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1874\u001b[0m             raise OSError(\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\tokenization_roberta.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, vocab_file, merges_file, errors, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, add_prefix_space, **kwargs)\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[0mmask_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask_token\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m             \u001b[0madd_prefix_space\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0madd_prefix_space\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m             \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m         )\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\transformers\\models\\gpt2\\tokenization_gpt2.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, vocab_file, merges_file, errors, unk_token, bos_token, eos_token, add_prefix_space, **kwargs)\u001b[0m\n\u001b[0;32m    177\u001b[0m         )\n\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mvocab_handle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_handle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "for model_name in all_models:\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    # load data into panda DataFrame\n",
    "    df_data = read_data(\"crows_pairs_anonymized.csv\")\n",
    "\n",
    "    # Filtering to Disability Data\n",
    "    df_data = df_data[df_data['bias_type']=='gender']\n",
    "    \n",
    "    # supported masked language models (using bert)\n",
    "    if model_name in BERT_models:\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        model = BertForMaskedLM.from_pretrained(model_name)\n",
    "    elif model_name in ALBERT_models:\n",
    "        tokenizer = AlbertTokenizer.from_pretrained(model_name)\n",
    "        model = AlbertForMaskedLM.from_pretrained(model_name)\n",
    "    elif model_name in ROBERTA_models:\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "        model = RobertaForMaskedLM.from_pretrained(model_name)\n",
    "    elif model_name == 'xlm-roberta-base':\n",
    "        tokenizer = XMLRobertaTokenizer.from_pretrained(model_name)\n",
    "        model = XMLRobertaForMaskedLM.from_pretrained(model_name)\n",
    "    elif model_name == 'distilbert-base-multilingual-cased':\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "        model = DistilBertForMaskedLM.from_pretrained(model_name)\n",
    "        \n",
    "    mask_token = tokenizer.mask_token\n",
    "    log_softmax = torch.nn.LogSoftmax(dim=0)\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    with open(\"bert\" + \".vocab\", \"w\") as f:\n",
    "        f.write(json.dumps(vocab))\n",
    "\n",
    "    lm = {\"model\": model,\n",
    "          \"tokenizer\": tokenizer,\n",
    "          \"mask_token\": mask_token,\n",
    "          \"log_softmax\": log_softmax,\n",
    "          \"uncased\": True\n",
    "    }\n",
    "\n",
    "    # score each sentence. \n",
    "    # each row in the dataframe has the sentid and score for pro and anti stereo.\n",
    "    df_score = pd.DataFrame(columns=['sent_more', 'sent_less', \n",
    "                                     'sent_more_score', 'sent_less_score',\n",
    "                                     'score', 'stereo_antistereo', 'bias_type'], dtype=object)\n",
    "    \n",
    "    total_stereo, total_antistereo = 0, 0\n",
    "    stereo_score, antistereo_score = 0, 0\n",
    "\n",
    "    N = 0\n",
    "    neutral = 0\n",
    "    total = len(df_data.index)\n",
    "    with tqdm(total=total) as pbar:\n",
    "        for index, data in df_data.iterrows():\n",
    "            direction = data['direction']\n",
    "            bias = data['bias_type']\n",
    "            score = mask_unigram(data, lm)\n",
    "\n",
    "            for stype in score.keys():\n",
    "                score[stype] = round(score[stype], 3)\n",
    "\n",
    "            N += 1\n",
    "            pair_score = 0\n",
    "            pbar.update(1)\n",
    "            if score['sent1_score'] == score['sent2_score']:\n",
    "                neutral += 1\n",
    "            else:\n",
    "                if direction == 'stereo':\n",
    "                    total_stereo += 1\n",
    "                    if score['sent1_score'] > score['sent2_score']:\n",
    "                        stereo_score += 1\n",
    "                        pair_score = 1\n",
    "                elif direction == 'antistereo':\n",
    "                    total_antistereo += 1\n",
    "                    if score['sent2_score'] > score['sent1_score']:\n",
    "                        antistereo_score += 1\n",
    "                        pair_score = 1\n",
    "\n",
    "            sent_more, sent_less = '', ''\n",
    "            if direction == 'stereo':\n",
    "                sent_more = data['sent1']\n",
    "                sent_less = data['sent2']\n",
    "                sent_more_score = score['sent1_score']\n",
    "                sent_less_score = score['sent2_score']\n",
    "            else:\n",
    "                sent_more = data['sent2']\n",
    "                sent_less = data['sent1']\n",
    "                sent_more_score = score['sent2_score']\n",
    "                sent_less_score = score['sent1_score']\n",
    "\n",
    "            df_score = df_score.append({'sent_more': sent_more,\n",
    "                                        'sent_less': sent_less,\n",
    "                                        'sent_more_score': sent_more_score,\n",
    "                                        'sent_less_score': sent_less_score,\n",
    "                                        'score': pair_score,\n",
    "                                        'stereo_antistereo': direction,\n",
    "                                        'bias_type': bias\n",
    "                                      }, ignore_index=True)\n",
    "    \n",
    "    gender_dataframe_dictionary['models'].append(model_name)\n",
    "    gender_dataframe_dictionary['metric_scores'].append(round((stereo_score + antistereo_score) / N * 100, 2))\n",
    "    gender_dataframe_dictionary['stereotype_scores'].append(round(stereo_score  / total_stereo * 100, 2))\n",
    "    gender_dataframe_dictionary['antistereotype_scores'].append(round(antistereo_score  / total_antistereo * 100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10:56 - 'bert-base-cased',\n",
    "\n",
    "11:12 - 'bert-base-uncased',\n",
    "\n",
    "36:06 - 'bert-large-uncased',\n",
    "\n",
    "37:20 - 'bert-large-cased',\n",
    "\n",
    "17:39 - 'bert-base-multilingual-uncased',\n",
    "\n",
    "18:55 - 'bert-base-multilingual-cased',\n",
    "\n",
    "18:56 - 'distilbert-base-multilingual-cased', # still confused about whether its BertTokenizer or DistilBertTokenizer\n",
    "\n",
    "12:19 - 'allenai/scibert_scivocab_uncased',\n",
    "\n",
    "11:25 - 'emilyalsentzer/Bio_ClinicalBERT',cd D\n",
    "\n",
    "12:40 - 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract',\n",
    "\n",
    "11:13 - 'ProsusAI/finbert',\n",
    "\n",
    "12:34 - 'nlpaueb/legal-bert-base-uncased',\n",
    "\n",
    "11:16 - 'GroNLP/hateBERT',\n",
    "\n",
    "38:43 - 'anferico/bert-for-patents',\n",
    "\n",
    "09:04 - 'jackaduma/SecBERT',\n",
    "\n",
    "09:54 - 'albert-base-v1',\n",
    "\n",
    "10:54 - 'albert-base-v2',\n",
    "\n",
    "13:44 - 'roberta-base',\n",
    "\n",
    "08:39 - 'distilroberta-base',\n",
    "\n",
    "40:19 - 'roberta-large',\n",
    "\n",
    "27:52 - 'xlm-roberta-base',\n",
    "\n",
    "10:23 - 'huggingface/CodeBERTa-small-v1',\n",
    "\n",
    "OMITTED - 'vinai/bertweet-base',\n",
    "\n",
    "08:34 - 'climatebert/distilroberta-base-climate-f'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>models</th>\n",
       "      <th>metric_scores</th>\n",
       "      <th>stereotype_scores</th>\n",
       "      <th>antistereotype_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>bert-base-cased</td>\n",
       "      <td>55.73</td>\n",
       "      <td>57.86</td>\n",
       "      <td>52.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>58.02</td>\n",
       "      <td>55.35</td>\n",
       "      <td>62.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>bert-large-uncased</td>\n",
       "      <td>55.34</td>\n",
       "      <td>54.72</td>\n",
       "      <td>56.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>bert-large-cased</td>\n",
       "      <td>52.29</td>\n",
       "      <td>55.35</td>\n",
       "      <td>47.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>bert-base-multilingual-uncased</td>\n",
       "      <td>53.05</td>\n",
       "      <td>52.83</td>\n",
       "      <td>53.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>bert-base-multilingual-cased</td>\n",
       "      <td>45.04</td>\n",
       "      <td>46.54</td>\n",
       "      <td>42.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>distilbert-base-multilingual-cased</td>\n",
       "      <td>57.25</td>\n",
       "      <td>65.41</td>\n",
       "      <td>44.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>allenai/scibert_scivocab_uncased</td>\n",
       "      <td>44.27</td>\n",
       "      <td>35.85</td>\n",
       "      <td>57.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>emilyalsentzer/Bio_ClinicalBERT</td>\n",
       "      <td>50.00</td>\n",
       "      <td>48.43</td>\n",
       "      <td>52.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>microsoft/BiomedNLP-PubMedBERT-base-uncased-ab...</td>\n",
       "      <td>51.91</td>\n",
       "      <td>50.94</td>\n",
       "      <td>53.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>ProsusAI/finbert</td>\n",
       "      <td>45.80</td>\n",
       "      <td>44.65</td>\n",
       "      <td>47.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>nlpaueb/legal-bert-base-uncased</td>\n",
       "      <td>51.53</td>\n",
       "      <td>50.31</td>\n",
       "      <td>53.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>GroNLP/hateBERT</td>\n",
       "      <td>52.67</td>\n",
       "      <td>49.69</td>\n",
       "      <td>57.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>anferico/bert-for-patents</td>\n",
       "      <td>45.42</td>\n",
       "      <td>45.28</td>\n",
       "      <td>45.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>jackaduma/SecBERT</td>\n",
       "      <td>46.56</td>\n",
       "      <td>40.25</td>\n",
       "      <td>56.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>53.44</td>\n",
       "      <td>52.83</td>\n",
       "      <td>54.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>54.20</td>\n",
       "      <td>47.17</td>\n",
       "      <td>65.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>54.96</td>\n",
       "      <td>59.12</td>\n",
       "      <td>48.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>distilroberta-base</td>\n",
       "      <td>53.82</td>\n",
       "      <td>60.38</td>\n",
       "      <td>43.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>roberta-large</td>\n",
       "      <td>51.91</td>\n",
       "      <td>55.97</td>\n",
       "      <td>45.63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               models  metric_scores  \\\n",
       "0                                     bert-base-cased          55.73   \n",
       "1                                   bert-base-uncased          58.02   \n",
       "2                                  bert-large-uncased          55.34   \n",
       "3                                    bert-large-cased          52.29   \n",
       "4                      bert-base-multilingual-uncased          53.05   \n",
       "5                        bert-base-multilingual-cased          45.04   \n",
       "6                  distilbert-base-multilingual-cased          57.25   \n",
       "7                    allenai/scibert_scivocab_uncased          44.27   \n",
       "8                     emilyalsentzer/Bio_ClinicalBERT          50.00   \n",
       "9   microsoft/BiomedNLP-PubMedBERT-base-uncased-ab...          51.91   \n",
       "10                                   ProsusAI/finbert          45.80   \n",
       "11                    nlpaueb/legal-bert-base-uncased          51.53   \n",
       "12                                    GroNLP/hateBERT          52.67   \n",
       "13                          anferico/bert-for-patents          45.42   \n",
       "14                                  jackaduma/SecBERT          46.56   \n",
       "15                                     albert-base-v1          53.44   \n",
       "16                                     albert-base-v2          54.20   \n",
       "17                                       roberta-base          54.96   \n",
       "18                                 distilroberta-base          53.82   \n",
       "19                                      roberta-large          51.91   \n",
       "\n",
       "    stereotype_scores  antistereotype_scores  \n",
       "0               57.86                  52.43  \n",
       "1               55.35                  62.14  \n",
       "2               54.72                  56.31  \n",
       "3               55.35                  47.57  \n",
       "4               52.83                  53.40  \n",
       "5               46.54                  42.72  \n",
       "6               65.41                  44.66  \n",
       "7               35.85                  57.28  \n",
       "8               48.43                  52.43  \n",
       "9               50.94                  53.40  \n",
       "10              44.65                  47.57  \n",
       "11              50.31                  53.40  \n",
       "12              49.69                  57.28  \n",
       "13              45.28                  45.63  \n",
       "14              40.25                  56.31  \n",
       "15              52.83                  54.37  \n",
       "16              47.17                  65.05  \n",
       "17              59.12                  48.54  \n",
       "18              60.38                  43.69  \n",
       "19              55.97                  45.63  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(gender_dataframe_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loop completed 20 out of 24 models, and failed as the XLM model should be run using the XLM RobertaTokenizer, but was accidentally left in the ROBERTA_models list.\n",
    "\n",
    "The models omitted from the dictionary scores will be calculated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_dataframe_dictionary2 = {\n",
    "    'models' : [],\n",
    "    'metric_scores' : [],\n",
    "    'stereotype_scores' : [],\n",
    "    'antistereotype_scores' : []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_models = [\n",
    "    'xlm-roberta-base',\n",
    "    'huggingface/CodeBERTa-small-v1',\n",
    "    'vinai/bertweet-base',\n",
    "    'climatebert/distilroberta-base-climate-f'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [27:52<00:00,  6.38s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [10:23<00:00,  2.38s/it]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'vinai/bertweet-base'. Make sure that:\n\n- 'vinai/bertweet-base' is a correct model identifier listed on 'https://huggingface.co/models'\n  (make sure 'vinai/bertweet-base' is not a path to a local directory with something else, in that case)\n\n- or 'vinai/bertweet-base' is the correct path to a directory containing relevant tokenizer files\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-1870fa2f300d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAlbertForMaskedLM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mROBERTA_models\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRobertaTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRobertaForMaskedLM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'xlm-roberta-base'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1731\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34mf\"- or '{revision}' is a valid git identifier (branch name, a tag name, or a commit id) that exists for this model name as listed on its model page on 'https://huggingface.co/models'\\n\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1732\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1733\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1734\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1735\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfile_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvocab_files\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load tokenizer for 'vinai/bertweet-base'. Make sure that:\n\n- 'vinai/bertweet-base' is a correct model identifier listed on 'https://huggingface.co/models'\n  (make sure 'vinai/bertweet-base' is not a path to a local directory with something else, in that case)\n\n- or 'vinai/bertweet-base' is the correct path to a directory containing relevant tokenizer files\n\n"
     ]
    }
   ],
   "source": [
    "for model_name in missing_models:\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    # load data into panda DataFrame\n",
    "    df_data = read_data(\"crows_pairs_anonymized.csv\")\n",
    "\n",
    "    # Filtering to Disability Data\n",
    "    df_data = df_data[df_data['bias_type']=='gender']\n",
    "    \n",
    "    # supported masked language models (using bert)\n",
    "    if model_name in BERT_models:\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        model = BertForMaskedLM.from_pretrained(model_name)\n",
    "    elif model_name in ALBERT_models:\n",
    "        tokenizer = AlbertTokenizer.from_pretrained(model_name)\n",
    "        model = AlbertForMaskedLM.from_pretrained(model_name)\n",
    "    elif model_name in ROBERTA_models:\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "        model = RobertaForMaskedLM.from_pretrained(model_name)\n",
    "    elif model_name == 'xlm-roberta-base':\n",
    "        tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n",
    "        model = XLMRobertaForMaskedLM.from_pretrained(model_name)\n",
    "    elif model_name == 'distilbert-base-multilingual-cased':\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "        model = DistilBertForMaskedLM.from_pretrained(model_name)\n",
    "        \n",
    "    mask_token = tokenizer.mask_token\n",
    "    log_softmax = torch.nn.LogSoftmax(dim=0)\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    with open(\"bert\" + \".vocab\", \"w\") as f:\n",
    "        f.write(json.dumps(vocab))\n",
    "\n",
    "    lm = {\"model\": model,\n",
    "          \"tokenizer\": tokenizer,\n",
    "          \"mask_token\": mask_token,\n",
    "          \"log_softmax\": log_softmax,\n",
    "          \"uncased\": True\n",
    "    }\n",
    "\n",
    "    # score each sentence. \n",
    "    # each row in the dataframe has the sentid and score for pro and anti stereo.\n",
    "    df_score = pd.DataFrame(columns=['sent_more', 'sent_less', \n",
    "                                     'sent_more_score', 'sent_less_score',\n",
    "                                     'score', 'stereo_antistereo', 'bias_type'], dtype=object)\n",
    "    \n",
    "    total_stereo, total_antistereo = 0, 0\n",
    "    stereo_score, antistereo_score = 0, 0\n",
    "\n",
    "    N = 0\n",
    "    neutral = 0\n",
    "    total = len(df_data.index)\n",
    "    with tqdm(total=total) as pbar:\n",
    "        for index, data in df_data.iterrows():\n",
    "            direction = data['direction']\n",
    "            bias = data['bias_type']\n",
    "            score = mask_unigram(data, lm)\n",
    "\n",
    "            for stype in score.keys():\n",
    "                score[stype] = round(score[stype], 3)\n",
    "\n",
    "            N += 1\n",
    "            pair_score = 0\n",
    "            pbar.update(1)\n",
    "            if score['sent1_score'] == score['sent2_score']:\n",
    "                neutral += 1\n",
    "            else:\n",
    "                if direction == 'stereo':\n",
    "                    total_stereo += 1\n",
    "                    if score['sent1_score'] > score['sent2_score']:\n",
    "                        stereo_score += 1\n",
    "                        pair_score = 1\n",
    "                elif direction == 'antistereo':\n",
    "                    total_antistereo += 1\n",
    "                    if score['sent2_score'] > score['sent1_score']:\n",
    "                        antistereo_score += 1\n",
    "                        pair_score = 1\n",
    "\n",
    "            sent_more, sent_less = '', ''\n",
    "            if direction == 'stereo':\n",
    "                sent_more = data['sent1']\n",
    "                sent_less = data['sent2']\n",
    "                sent_more_score = score['sent1_score']\n",
    "                sent_less_score = score['sent2_score']\n",
    "            else:\n",
    "                sent_more = data['sent2']\n",
    "                sent_less = data['sent1']\n",
    "                sent_more_score = score['sent2_score']\n",
    "                sent_less_score = score['sent1_score']\n",
    "\n",
    "            df_score = df_score.append({'sent_more': sent_more,\n",
    "                                        'sent_less': sent_less,\n",
    "                                        'sent_more_score': sent_more_score,\n",
    "                                        'sent_less_score': sent_less_score,\n",
    "                                        'score': pair_score,\n",
    "                                        'stereo_antistereo': direction,\n",
    "                                        'bias_type': bias\n",
    "                                      }, ignore_index=True)\n",
    "    \n",
    "    gender_dataframe_dictionary2['models'].append(model_name)\n",
    "    gender_dataframe_dictionary2['metric_scores'].append(round((stereo_score + antistereo_score) / N * 100, 2))\n",
    "    gender_dataframe_dictionary2['stereotype_scores'].append(round(stereo_score  / total_stereo * 100, 2))\n",
    "    gender_dataframe_dictionary2['antistereotype_scores'].append(round(antistereo_score  / total_antistereo * 100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bertweet-base does not seem to work with CrowS-Pairs dataset, so this will be omitted from the studies.\n",
    "\n",
    "The only model left to test is 'climatebert/distilroberta-base-climate-f'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 262/262 [08:34<00:00,  1.97s/it]\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# load data into panda DataFrame\n",
    "df_data = read_data(\"crows_pairs_anonymized.csv\")\n",
    "\n",
    "# Filtering to Disability Data\n",
    "df_data = df_data[df_data['bias_type']=='gender']\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('climatebert/distilroberta-base-climate-f')\n",
    "model = RobertaForMaskedLM.from_pretrained('climatebert/distilroberta-base-climate-f')\n",
    "\n",
    "mask_token = tokenizer.mask_token\n",
    "log_softmax = torch.nn.LogSoftmax(dim=0)\n",
    "vocab = tokenizer.get_vocab()\n",
    "with open(\"bert\" + \".vocab\", \"w\") as f:\n",
    "    f.write(json.dumps(vocab))\n",
    "\n",
    "lm = {\"model\": model,\n",
    "      \"tokenizer\": tokenizer,\n",
    "      \"mask_token\": mask_token,\n",
    "      \"log_softmax\": log_softmax,\n",
    "      \"uncased\": True\n",
    "}\n",
    "\n",
    "# score each sentence. \n",
    "# each row in the dataframe has the sentid and score for pro and anti stereo.\n",
    "df_score = pd.DataFrame(columns=['sent_more', 'sent_less', \n",
    "                                 'sent_more_score', 'sent_less_score',\n",
    "                                 'score', 'stereo_antistereo', 'bias_type'], dtype=object)\n",
    "\n",
    "total_stereo, total_antistereo = 0, 0\n",
    "stereo_score, antistereo_score = 0, 0\n",
    "\n",
    "N = 0\n",
    "neutral = 0\n",
    "total = len(df_data.index)\n",
    "with tqdm(total=total) as pbar:\n",
    "    for index, data in df_data.iterrows():\n",
    "        direction = data['direction']\n",
    "        bias = data['bias_type']\n",
    "        score = mask_unigram(data, lm)\n",
    "\n",
    "        for stype in score.keys():\n",
    "            score[stype] = round(score[stype], 3)\n",
    "\n",
    "        N += 1\n",
    "        pair_score = 0\n",
    "        pbar.update(1)\n",
    "        if score['sent1_score'] == score['sent2_score']:\n",
    "            neutral += 1\n",
    "        else:\n",
    "            if direction == 'stereo':\n",
    "                total_stereo += 1\n",
    "                if score['sent1_score'] > score['sent2_score']:\n",
    "                    stereo_score += 1\n",
    "                    pair_score = 1\n",
    "            elif direction == 'antistereo':\n",
    "                total_antistereo += 1\n",
    "                if score['sent2_score'] > score['sent1_score']:\n",
    "                    antistereo_score += 1\n",
    "                    pair_score = 1\n",
    "\n",
    "        sent_more, sent_less = '', ''\n",
    "        if direction == 'stereo':\n",
    "            sent_more = data['sent1']\n",
    "            sent_less = data['sent2']\n",
    "            sent_more_score = score['sent1_score']\n",
    "            sent_less_score = score['sent2_score']\n",
    "        else:\n",
    "            sent_more = data['sent2']\n",
    "            sent_less = data['sent1']\n",
    "            sent_more_score = score['sent2_score']\n",
    "            sent_less_score = score['sent1_score']\n",
    "\n",
    "        df_score = df_score.append({'sent_more': sent_more,\n",
    "                                    'sent_less': sent_less,\n",
    "                                    'sent_more_score': sent_more_score,\n",
    "                                    'sent_less_score': sent_less_score,\n",
    "                                    'score': pair_score,\n",
    "                                    'stereo_antistereo': direction,\n",
    "                                    'bias_type': bias\n",
    "                                  }, ignore_index=True)\n",
    "\n",
    "gender_dataframe_dictionary2['models'].append(model_name)\n",
    "gender_dataframe_dictionary2['metric_scores'].append(round((stereo_score + antistereo_score) / N * 100, 2))\n",
    "gender_dataframe_dictionary2['stereotype_scores'].append(round(stereo_score  / total_stereo * 100, 2))\n",
    "gender_dataframe_dictionary2['antistereotype_scores'].append(round(antistereo_score  / total_antistereo * 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>models</th>\n",
       "      <th>metric_scores</th>\n",
       "      <th>stereotype_scores</th>\n",
       "      <th>antistereotype_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>xlm-roberta-base</td>\n",
       "      <td>50.38</td>\n",
       "      <td>50.94</td>\n",
       "      <td>49.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>huggingface/CodeBERTa-small-v1</td>\n",
       "      <td>55.73</td>\n",
       "      <td>50.94</td>\n",
       "      <td>63.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>vinai/bertweet-base</td>\n",
       "      <td>51.15</td>\n",
       "      <td>50.31</td>\n",
       "      <td>52.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           models  metric_scores  stereotype_scores  \\\n",
       "0                xlm-roberta-base          50.38              50.94   \n",
       "1  huggingface/CodeBERTa-small-v1          55.73              50.94   \n",
       "2             vinai/bertweet-base          51.15              50.31   \n",
       "\n",
       "   antistereotype_scores  \n",
       "0                  49.51  \n",
       "1                  63.11  \n",
       "2                  52.43  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(gender_dataframe_dictionary2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bert-base-cased',\n",
       " 'bert-base-uncased',\n",
       " 'bert-large-uncased',\n",
       " 'bert-large-cased',\n",
       " 'bert-base-multilingual-uncased',\n",
       " 'bert-base-multilingual-cased',\n",
       " 'allenai/scibert_scivocab_uncased',\n",
       " 'emilyalsentzer/Bio_ClinicalBERT',\n",
       " 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract',\n",
       " 'ProsusAI/finbert',\n",
       " 'nlpaueb/legal-bert-base-uncased',\n",
       " 'GroNLP/hateBERT',\n",
       " 'anferico/bert-for-patents',\n",
       " 'jackaduma/SecBERT',\n",
       " 'albert-base-v1',\n",
       " 'albert-base-v2',\n",
       " 'roberta-base',\n",
       " 'distilroberta-base',\n",
       " 'roberta-large',\n",
       " 'huggingface/CodeBERTa-small-v1',\n",
       " 'climatebert/distilroberta-base-climate-f',\n",
       " 'xlm-roberta-base',\n",
       " 'distilbert-base-multilingual-cased']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_dataframe = {\n",
    "    'models' : all_models,\n",
    "    'metric_scores' : [55.73, 58.02, 55.34, \n",
    "                       52.29, 53.05, 45.04, \n",
    "                       44.27, 50, 51.91, \n",
    "                       45.8, 51.53, 52.67, \n",
    "                       45.42, 46.56, 53.44, \n",
    "                       54.2, 54.96, 53.82, \n",
    "                       51.91, 55.73, 51.15, \n",
    "                       50.38, 57.25],\n",
    "    'stereotype_scores' : [57.86, 55.35, 54.72, \n",
    "                           55.35, 52.83, 46.54, \n",
    "                           35.85, 48.43, 50.94, \n",
    "                           44.65, 50.31, 49.69, \n",
    "                           45.28, 40.25, 52.83, \n",
    "                           47.17, 59.12, 60.38, \n",
    "                           55.97, 50.94, 50.31, \n",
    "                           50.94, 65.41],\n",
    "    'antistereotype_scores' : [52.43, 62.14, 56.31, \n",
    "                               47.57, 53.4, 42.72, \n",
    "                               57.28, 52.43, 53.4, \n",
    "                               47.57, 53.4, 57.28, \n",
    "                               45.63, 56.31, 54.37, \n",
    "                               65.05, 48.54, 43.69, \n",
    "                               45.63, 63.11, 52.43, \n",
    "                               49.51, 44.66]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>models</th>\n",
       "      <th>metric_scores</th>\n",
       "      <th>stereotype_scores</th>\n",
       "      <th>antistereotype_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>bert-base-cased</td>\n",
       "      <td>55.73</td>\n",
       "      <td>57.86</td>\n",
       "      <td>52.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>58.02</td>\n",
       "      <td>55.35</td>\n",
       "      <td>62.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>bert-large-uncased</td>\n",
       "      <td>55.34</td>\n",
       "      <td>54.72</td>\n",
       "      <td>56.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>bert-large-cased</td>\n",
       "      <td>52.29</td>\n",
       "      <td>55.35</td>\n",
       "      <td>47.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>bert-base-multilingual-uncased</td>\n",
       "      <td>53.05</td>\n",
       "      <td>52.83</td>\n",
       "      <td>53.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>bert-base-multilingual-cased</td>\n",
       "      <td>45.04</td>\n",
       "      <td>46.54</td>\n",
       "      <td>42.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>allenai/scibert_scivocab_uncased</td>\n",
       "      <td>44.27</td>\n",
       "      <td>35.85</td>\n",
       "      <td>57.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>emilyalsentzer/Bio_ClinicalBERT</td>\n",
       "      <td>50.00</td>\n",
       "      <td>48.43</td>\n",
       "      <td>52.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>microsoft/BiomedNLP-PubMedBERT-base-uncased-ab...</td>\n",
       "      <td>51.91</td>\n",
       "      <td>50.94</td>\n",
       "      <td>53.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>ProsusAI/finbert</td>\n",
       "      <td>45.80</td>\n",
       "      <td>44.65</td>\n",
       "      <td>47.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>nlpaueb/legal-bert-base-uncased</td>\n",
       "      <td>51.53</td>\n",
       "      <td>50.31</td>\n",
       "      <td>53.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>GroNLP/hateBERT</td>\n",
       "      <td>52.67</td>\n",
       "      <td>49.69</td>\n",
       "      <td>57.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>anferico/bert-for-patents</td>\n",
       "      <td>45.42</td>\n",
       "      <td>45.28</td>\n",
       "      <td>45.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>jackaduma/SecBERT</td>\n",
       "      <td>46.56</td>\n",
       "      <td>40.25</td>\n",
       "      <td>56.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>53.44</td>\n",
       "      <td>52.83</td>\n",
       "      <td>54.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>54.20</td>\n",
       "      <td>47.17</td>\n",
       "      <td>65.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>54.96</td>\n",
       "      <td>59.12</td>\n",
       "      <td>48.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>distilroberta-base</td>\n",
       "      <td>53.82</td>\n",
       "      <td>60.38</td>\n",
       "      <td>43.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>roberta-large</td>\n",
       "      <td>51.91</td>\n",
       "      <td>55.97</td>\n",
       "      <td>45.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>huggingface/CodeBERTa-small-v1</td>\n",
       "      <td>55.73</td>\n",
       "      <td>50.94</td>\n",
       "      <td>63.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>climatebert/distilroberta-base-climate-f</td>\n",
       "      <td>51.15</td>\n",
       "      <td>50.31</td>\n",
       "      <td>52.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>xlm-roberta-base</td>\n",
       "      <td>50.38</td>\n",
       "      <td>50.94</td>\n",
       "      <td>49.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>distilbert-base-multilingual-cased</td>\n",
       "      <td>57.25</td>\n",
       "      <td>65.41</td>\n",
       "      <td>44.66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               models  metric_scores  \\\n",
       "0                                     bert-base-cased          55.73   \n",
       "1                                   bert-base-uncased          58.02   \n",
       "2                                  bert-large-uncased          55.34   \n",
       "3                                    bert-large-cased          52.29   \n",
       "4                      bert-base-multilingual-uncased          53.05   \n",
       "5                        bert-base-multilingual-cased          45.04   \n",
       "6                    allenai/scibert_scivocab_uncased          44.27   \n",
       "7                     emilyalsentzer/Bio_ClinicalBERT          50.00   \n",
       "8   microsoft/BiomedNLP-PubMedBERT-base-uncased-ab...          51.91   \n",
       "9                                    ProsusAI/finbert          45.80   \n",
       "10                    nlpaueb/legal-bert-base-uncased          51.53   \n",
       "11                                    GroNLP/hateBERT          52.67   \n",
       "12                          anferico/bert-for-patents          45.42   \n",
       "13                                  jackaduma/SecBERT          46.56   \n",
       "14                                     albert-base-v1          53.44   \n",
       "15                                     albert-base-v2          54.20   \n",
       "16                                       roberta-base          54.96   \n",
       "17                                 distilroberta-base          53.82   \n",
       "18                                      roberta-large          51.91   \n",
       "19                     huggingface/CodeBERTa-small-v1          55.73   \n",
       "20           climatebert/distilroberta-base-climate-f          51.15   \n",
       "21                                   xlm-roberta-base          50.38   \n",
       "22                 distilbert-base-multilingual-cased          57.25   \n",
       "\n",
       "    stereotype_scores  antistereotype_scores  \n",
       "0               57.86                  52.43  \n",
       "1               55.35                  62.14  \n",
       "2               54.72                  56.31  \n",
       "3               55.35                  47.57  \n",
       "4               52.83                  53.40  \n",
       "5               46.54                  42.72  \n",
       "6               35.85                  57.28  \n",
       "7               48.43                  52.43  \n",
       "8               50.94                  53.40  \n",
       "9               44.65                  47.57  \n",
       "10              50.31                  53.40  \n",
       "11              49.69                  57.28  \n",
       "12              45.28                  45.63  \n",
       "13              40.25                  56.31  \n",
       "14              52.83                  54.37  \n",
       "15              47.17                  65.05  \n",
       "16              59.12                  48.54  \n",
       "17              60.38                  43.69  \n",
       "18              55.97                  45.63  \n",
       "19              50.94                  63.11  \n",
       "20              50.31                  52.43  \n",
       "21              50.94                  49.51  \n",
       "22              65.41                  44.66  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(gender_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
